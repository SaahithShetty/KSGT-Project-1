{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (7.1.3)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (0.11.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (3.1.4)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install rdflib nltk pdfplumber scikit-learn spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Existing Classes: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/EthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InformationProcessing', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Interaction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Scenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Capability']\n",
      "‚úÖ Existing Object Properties: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/capability', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasEthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasInteraction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/inScenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/informationMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactingAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingInformation', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingTask']\n",
      "‚úÖ Existing Data Properties: []\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, OWL, RDFS\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"  # Update with the correct path\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "\n",
    "# Extract classes\n",
    "existing_classes = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.Class)]\n",
    "\n",
    "# Extract object properties\n",
    "existing_object_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "# Extract data properties\n",
    "existing_data_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "# Print summary\n",
    "print(\"‚úÖ Existing Classes:\", existing_classes)\n",
    "print(\"‚úÖ Existing Object Properties:\", existing_object_properties)\n",
    "print(\"‚úÖ Existing Data Properties:\", existing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Extracted Key Concepts:\n",
      "üìå Top Keywords: ['trust', 'ai', 'agent', 'rl', 'design', 'learning', 'human', 'acc', 'user', 'pvossenetalaconversationalagent', 'sampling', 'figure', 'cid', 'available', 'al', 'data', 'model', 'cluster', 'uncertainty', 'wtchangetalhumancenteredaifordementiacare']\n",
      "üìå Named Entities: ['Reinforcement Learning', '0.002 0.173 0.0 0.0 0.0', 'QR-Code', 'PatrizioPelliccione', 'DST', 'International\\nJournal of Mental Health', 'UsingaKruskal-Wallisranksum', 'Train/Val/Testsize', 'Benda', 'Boehm-DavisDA.EffectsofAgeandCongestionInformationAccuracyofAdvancedTraveler\\nInformationSystemsonUserTrustandCompliance', 'Atari', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 353', 'NormanSadeh', 'SantaClara', 'NY', 'SunY', 'MDPandtrytolearnahigherperformingpolicy', 'AI', 'CHI Conference', '0.716 0.0', 'TheRoleofTrustin\\nHuman-RobotInteraction', '0.776 0.487', '0.855 0.631 0.51 0.341', 'World Health Organization', 'Fig.1', 'EUR', 'Acc <Acc', 'Massimiliano', 'Construct QualitativeCodes QuantitativeMeasures(DataRange', 'DZ', 'CaoS', 'Informatics', 'Forthisreason', 'BoneXpert', 'Florence, Italy: Association for Computational Linguistics', 'Idonothavetheabilitytoadjusttoindividualpreferencesintheway', 'SPIE\\nProceedings', 'the 21st International Conference on Intelligent User Interfaces', 'Categorical Data', 'Platt JE', 'Artificial Intelligence (AI', 'MichaelMuller', 'Althoefer\\nK', 'Bernardino A', 'Trust Through the AI Development Team', '.082)abc p<.0001\\nConscientiousness', 'CHI\\nConference on Human Factors', 'Frontiers of Theory and Research', 'asdescribedintheAI-TAM', 'Fitzpatrick', 'DiscussionandConclusion\\nInthiswork', 'Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence', 'Dazeley R', 'Cham,2022', 'Conscientiousness', 'LiL', 'BMC Med Inform Decis Mak', 'PatrizioMigliarini', 'AnupamDas', 'Bart', 'Wealsosimulatedcriticaleventsduringtherouteplanningtask', 'SomayaBENALLOUCH', 'Hybrid Human AI Systems', 'ProceedingsoftheACM\\nonInteractive', 'Philosophy & Technology', 'Functioning & Well-Being\\nPiekVOSSENa,1,SeleneBA¬¥EZSANTAMAR¬¥IAaandThomasBAIERa', 'the Dutch Ministry of Education', 'Analyse', 'SC', 'ElianeSommerfeld', 'Trust Through Clinical Involvement', 'Legal Theory', 'TOS', 'F for s¬Ø', 'GuidelinesforHuman-AIInteractionwhichseekstoguidedinteractionovertimebeyond', '0.147 0.018 0.795', 'means‚Äô[10,15', 'Ashktorab Z', 'Social Psychology Bulletin,12(1):75‚Äì80,1986', 'Reflexive Thematic Analysis', 'the Certifiably Optimal Rule Lists', 'International Journal', 'the\\nEuropean Union‚Äôs', 'Computing Systems', 'AntinoKim', 'RahmaniAslM. SketchOpt', 'NASA', 'NONDET', 'EXOSOULisbasedonthenotion', 'al', 'DeepEpsilon-Greedy', 'MoharebM', 'A ‚â• Acc', 'European Commission', 'US Airlines', 'Ferna¬¥ndez-LealA.Human-', 'Smith-Renner A', 'AishaAkbar', 'Post-Deployment\\n3.5.1', 'AnEarly\\nDesign Framework to Support Concept Creation and Evaluation', 'Wilcox L', 'Gutzwiller RS', 'Antecedents of Trust', 'W.-T.Changetal./Human', 'McGraw-Hill', 'AI & law', '.315 F', 'BaselineResults', 'DigitalBehaviors', 'manexpert‚Äôswillingnesstocontributetotheco-creativetoolduringandafterco-creation', 'weuseencoderLLMsfine', 'F :', 'contrastwiththistraditionalviewofRL', 'GDPR', 'NWO', 'Clusteranalysis.newberrypark,1984', 'Utrecht University', 'LucianoFloridi', 'Trust Through the Training of the AI', 'UserBot', 'CB', 'Artificial Intelligence, Trust Calibration', 'Corollary 5', 'CBR', 'r(x', '‚Ä¢ (Thenumberofdesigngoals', 'PwD', 'InInternational Conference', 'Robben S', '0.96', 'the Base Samples of Cognos Analytics', 'Pringle C, Eleftheriou I', 'Armaan A', 'Race', 'NiklasKuehl', 'AI\\nlearning(r=.74', 'International Journal of Human', '0.443 0.078 0.071', 'Siewiorek DP', 'De Keizer NF', 'Scheffler', 'AAAIPress', 'USA\\nAbstract', 'Hexaco', 'Journal of Applied', 'MichaelDesmond', 'HjorthA', 'YunfengZhang', 'Ackerman MS', 'BinLiu', 'CarinaBenz', 'GrossMD', 'AI-TAM', 'Costanza', 'SME', 'U.S. Food and Drug Administration', 'Kerstholt JH', 'JoshuaDMiller', 'Martic M', 'ADL', 'LiH', 'Exosoul', 'User Profiling', 'AIsystemandemphasizetheimportanceofmaintaininguserengagementandreducing', 'DensityandSaturationcriteria Eventsandtheirpropertiescanbenecessary', 'Discovery', 'un', 'Cummings ML', 'Open Access', 'TimeCollaboration', 'Santos-Rodr¬¥ƒ±guezR', 'CHI Conference on Human Factors', 'Thiscodecould ‚Ä¢ Idonotfeeloutofcontrolinworkingwithit', 'EXOSOUL Ethical Engine Scheme', 'x‚àó=argmin', 'JMIR Med Inform', 'the ACM on Human-Computer Interaction', 'Aysen Gurcan Namlu', 'Strohm', 'therecentlylaunchedChatGPTseemstoprovidehuman', 'the University of L‚ÄôAquila', 'Davidoff S. Trust', 'AarhusUniversity\\nAbstract', 'the Dark Triad Dirty', 'the EFMI Working Group for Assessment of Health Information Systems', 'Trust Through Clinical Trials and Peer Reviewed Publications\\nPrior', 'Novak LL', 'Pre-Development', 'Journal of personality,83(6):644‚Äì664,2015', 'McDaniel', 'Sutton RS', 'Cole', 'Adams R', 'LiuY', 'EPT', 'Yearb Med Inform', 'BMC Med', 'the Small COMPAS Data', 'Defining Trust', 'Koenigsberg MR', '7.00', 'Correctional Offender Management Profiling', 'Deep Epsilon-Greedy', 'CTSachievedthebestperformanceamongthealgorithms', 'CERTAINTY 5', 'Yearb\\nMed Inform', 'PimentelJ', 'Wang', 'N', 'The Pearson Correlation', 'Inthisbalanceddataset', 'ACMTransactions', 'Leeuwenberg AM', 'MMOLAEE', 'Expanding Trust', 'aUniversit√†', 'LiuX', 'Dulac-ArnoldG', 'Healthcare', 'KellyT', 'Tukey‚Äôs Post-hoc', 'Bedoya AD', 'the Creative Commons Attribution Non-Commercial License', 'Acc ‚àíA >0.Theminimumachievabledecision', 'Prokop M', 'Neural Information Processing Systems', '0.076', 'User Modeling', 'A European Review', 'Monitoringpatientswell', 'Initial Ethical\\nInference Engine Profile', 'inwhichthemodelneverabstains(k=0).However', 'Computing Sciences', 'the Clinical AI Deployment Cycle', 'NewYork', 'Department of Information', 'ThomasJ', 'ICCIDS', 'Cabitza F', 'Black AW', 'Multimodal Semantic Representations', 'Ernest H O‚ÄôBoyle', 'n‚Üí‚àû', 'Acc >Acc', '0.641 0.456 0.237', 'BTOS', 'Ourcontributiontohuman-AIin-', 'RobinsonL.Personalisation', 'F :s.', 'Workload', 'Keshmiri S', 'Chernova S', 'Thispaperdiscusses5stepstowardssuccessfulRLdeployment', 'the Netherlands\\nOrganisation for Scientific Research', 'Informatics Institute', 'User Trust', 'Computational Models of Argument', 'Asourexperimentalevaluationwillshow', 'GroveAJ', 'Pak R', 'MadeleineGrunde-McLaughlin', 'Table 4.3', 'University of Amsterdam', 'Tejeda', 'Ourcontributiontohuman-AIinteractionisthree-fold', 'Digital', 'NgAY', 'Kramer RM', 'DoEYL', 'doi.org/10.1109/IROS.2017.8202133', 'AndreaPASSERINIaandFabioCASATIc', 'Logistic Regression', 'Horvat CM', 'Pearson', 'the 2020 Conference on\\nFairness, Accountability', 'Nuthakki S', 'Parker W', 'PadhraicSmyth', 'WieseE', 'Ontological References', 'Grisel', 'Label 0', '0.392', 'Conf\\nProc Ethnogr Prax Ind Conf', 'the AAAI Conference on Human Computation\\n', 'F:TrafficPsychologyandBehaviour.2020;73:15-28', 'Curran\\nAssociates', 'the Interdependence of Reliance\\nBehavior', 'ML', 'Procedia', 'USA,2020.AssociationforComputingMachinery', 'IBM SPSS Statistics', 'Remark 4.1', 'CRC Press', 'totrainwasassociatedwithmorewillingness(r=.72', 'Journal of Research', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 357', 'ContextualandNon', 'AIaccuracy', 'F, Tischler V', 'the Netherlands\\nAbstract', 'p<.001‚àó‚àó‚àó)andAIoutputtrust(r=.76', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches', '0.632 0.551 0.385', 'GD', '‚Ä¢ Q2', 'correlation[48', 'Sendak', 'Adaptation and Personalization', 'Fussell SR', 'Caetano', 'HelenaVasconcelos', 'Conversationalintent\\nFortheagentunderconsideration', 'GP', 'MieleD', 'Automated Systems', 'Anova', 'CIFARAIChair', 'Ely JW', 'CÀáyras K', 'Incontrast', 'Official Journal', 'LiK', 'Autonomous Systems', 'KimJ', 'Human Preferences', 'Povykalo A', 'Delft University of Technology', '‚Ä¢ Certainty', 'linear', 'CTS 1', 'Acc ‚àà[10%,50%', 'Argument & Computation', 'Harv JL & Tech', 'J Am Med Inform\\nAssoc', 'TomBuchanan', 'Grasshopper', 'andFlorianSchaub', 'Radiol Med', 'TAM', 'Wenamethisstrategythreshold-orientedsampling(TOS).Note', 'Acosta-ZazuetaG.A3Dshapegenerativemethodforaestheticprod-', 'AI\\nGeometrically', 'x‚àó=argmin x‚ààD', 'Swift Trust', 'Hum Factors', 'The European Parliament', '‚Ä¢ Ithinkitisuselesstotrainitwithmylikes', 'ChargeDegree F M\\nForced', 'MartinDegeling', 'RobertLoftin', 'Albayram', 'Nijman SWJ', 'cancelevel:‚àó‚àó‚àóp<.001,‚àó‚àóp<.01,‚àóp<.05', 'Admissions', 'Marsh S', 'People Business SME DevOps Laypeople ML/RL', 'CA', 'Muir', 'QualitativeCo-Occurrences', 'Thelocationmapislaidonagrid', 'LimitationsandConclusions\\nInthiswork', 'ProPublica', 'Keebler JR', '0.51 0.493 0.198', 'CTS', 'Delroy L Paulhus', '323', 'LauraPetrich', 'InthiscontextthemultidisciplinaryprojectExosoul', 'P7', 'F ‚â∫ M', 'Antaki JF', 'Community', 'Science of Computer', 'Computing\\nSystems', 'HCOMP', '0.079 0.034 0.971', 'Holloway A', 'Inference Engine\\nFill', 'Reliance', 'NPJ Digit Med', 'SocialScience', 'n‚ààN', 'Working Group', 'Singh P, Pomerantz S, Doyle S, Kakarmath S', 'ACM Conference on Computer Supported Cooperative Work\\n', 'LeeJD.ModelingGoalAlignmentinHuman-AITeaming', 'TREWS', 'Building Trust', 'MDP', 'Feedback Interface', 'Amsterdam University of Applied Sciences', 'Niklas', 'ACM', 'VanDuinC', 'Theparticipantcouldinter-\\nvenebyclickingtheInterruptbutton', 'Acc', 'Functioning', 'Business Ethics', 'Dark Triad', 'Riener A', 'hasPlace', 'EHR', 'ArturNilsson', 'ancinghowimpactfulasuccessfulsolutionwillbe', 'Trust Through a Clinical Champion\\nClinical', '0.031 0.777', 'COMPAS', 'forthedeterministic(D)andnon', 'Florence L Geis', 'ChengCY', 'CAD', 'RLOps', 'Wilson S, Stumpf S. User', 'theparticipantstrustedandusedtheGPSdrivingadvice', 'Knowledge-Based Systems', 'Larus-Stone N', 'J Am Med Inform Assoc', 'Value-Aware Active Learning', 'ChuW', 'European Dementia Monitor', 'Nature Machine Intelligence', '‚Ä¢ Wouldyouinvestyourtimeintrainingtouse', 'Ahuman', 'Taya R Cohen', 'the Ethics Position Questionnaire', 'Komorowski M', 'International Journal of Management and Bussiness', 'Med Care Res Rev', 'HAI', '0.04', '0.144 0.777 0.335', 'NPD-TAM', 'Veloso M', 'SchapireRE', 'Antony AS', 'AssociationforComputing', 'the 2020 Conference on Fairness, Accountability', 'EU', 'Drinking Scenarios\\nWen-TsengCHANGa', 'F', 'AI@Trento(FBK-Unitn', 's(x)‚â§œÑ(œÑ‚àís(x', 'RL', 'Horizon 2020', '‚é™‚é™‚é®(A correct+O correct)‚àí(100%‚àíAcc AI‚àíA', 'McNair JB', 'Table 2', 'the Unit of Analysis of Trust', 'Instagram', 'Dimensions, Preferences', 'IBM', 'communication,17(1):1‚Äì18,2011', 'Social Computing', 'the Council of the European Union', 'Burcu Sayin', 'Northpointe Inc.', '6.04 7.00', 'LiY', 'AI\\nW', 'Black-Box Systems', 'LLM', 'F :s', 'USA: Association', 'CheXplain', 'Daronnat S', 'Cece', '0.045 0.02', 'Success', 'ChristinaGeary', 'MaxSchemmer', 'CeliaMoore', 'LeeS', 'Linear Upper Confidence Bound', 'Academy of Management Journal', 'HateSpeech', 'Complianceisoftenusedasabehaviouraldemonstrationoftrustinhuman', 'AI AI AI', 'FactBank', 'Bellamy RKE', 'Alessandro Acquisti', 'http://dx.doi.org/10.1145/3181671', 'andempathicAIsystemthatcan', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 359\\nReferences', 'FDA', 'betweenthelandmarks(œá2=125.36', 'Business\\n&InformationSystemsEngineering.2016;58:367-', 'Nishikawa', 'CTSdynamicallyadaptsitsapproach', 'Paul T Costa', 'Current Psychology', 'Ward', 'LiP', 'JP', 'cid:3', 'NotethatsinceAcc', 'MarcoAutili', 'Agrawal M', 'Hoesterey S', 'Personal Differences', 'andContextualThompsonSampling(CTS)[24].Amongthem', 'Fahim MAA', 'Machine\\nLearning in Python', 'El≈ºbieta Sanecka', 'PatrickHemmer', 'VirtualAssistants-TheApplication\\nBroadly', 'ambi-\\nent', 'Ingeneral', 'ActivitiesofDailyLife(ADL)[1,2,3].Suchmonitoringis', 'object‚Üíspace', 'PaolaInverardi', 'Narcissism', 'Softethicsandthegovernanceofthedigital', 'AdrianFurnham', 'Pareek A', 'Clinical AI', 'HRI', 'M', 'Computer', '0.74', 'GrainedEmotions', 'Hegde N', 'AutonomousSystems', 'Keywords', 'the Google Maps-assisted', 'Northepointe, Inc.', 'Thompson WR', 'LuJ', 'AdrianoSchimmenti', 'The Journal of\\nEducation', 'Phillips EK', 'B√ºy√ºkt√ºr AG', 'EvitaMarch', 'ETP', 'l', '‚Ä¢ Itisverycapableofperformingitsjob', 'Netherlands Organisation for Scientific Research', 'Graber ML', 'SchoolofBusinessand', 'KlasnjaP', 'UnitedKingdom', 'IEEE', 'International Data Privacy Law', 'Reale C', 'TheAcademyof', 'Personality,43(5):747‚Äì754,2009', 'Distributed Dynamic\\nTeam Trust', 'TrustinHuman-Robot\\nInteraction', 'JALT Testing & Evaluation SIG Newsletter', 'andneedsofPwD', 'Teller Road', 'IOS Press', 'AI\\nAcc ‚àà', 'American Institute of Aeronautics', 'StatisticsAnova', 'Individual Differences,100:85‚Äì94,2016', 'University of Groningen', 'dictingdigitalbehaviorsindifferentagesamples(e.g', 'Hatice Ferhan Odabasi', 'BMJ Health Care Inform', 'Harvard\\nData Science Review', 'Dementia', 'wewouldexpectthehumantoadheretoA%ofcorrectAIrecommendationsandA%', 'ChargeDegree Indicates', 'DCODE', 'the COMPAS Recidivism Algorithm', 'MochenYang', 'Amodei D. Deep Reinforcement', 'CarinaPaine', 'GreenewaldK', 'weshowthatthehumanmayneverbeexpectedtocomplementtheAI', 'Artificial Intelligence - A', 'SPIE', 'BMC Med Inform Decis\\nMak', 'Certainty', '0.002 0.0', 'Sepsis Watch', 'Krishnaswamy N', 'DesigningforHuman-CenteredSystems', 'AakritiKumar', 'Digital Humanism', 'Paul T Costa Jr', 'Nat Med', 'Kiesler S', 'Khodyakov D. Trust', 'JoshAndres', 'KeenJ', 'Journal of Machine Learning Research', 'a Philips Experience Design', 'ConsideringthecognitiveimpairmentofPwD', 'Alirezaie M', 'Racedimension', 'forDET', 'AcademicPress;2021.p.143', 'DET', 'ArtificialIntelligence.1999;112(1):181-211', 'Clare AS', 'Center for Devices, Radiological Health', 'AungMH', 'the Hybrid Intelligence Center', 'Barda AJ', 'Accfinal', 'UsingaBelieve-Desire-Intentprotocol[20],theplatformutilizesmodulesthatde-', 'Intelligent Systems Technical Conference', 'Trust Through Public Accountability', 'JournalofHumanNutritionandDietetics.2022;35(1):165', 'Machine Learning', 'CSCW', 'PiersSteel']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    extracted_texts = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \" \".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                extracted_texts.append(text)\n",
    "    return extracted_texts\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Extract key terms using TF-IDF\n",
    "def extract_keywords_tfidf(texts, num_keywords=20):\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words=\"english\", ngram_range=(1,2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
    "    keyword_scores = dict(zip(feature_names, tfidf_scores))\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [keyword for keyword, _ in sorted_keywords[:num_keywords]]\n",
    "\n",
    "# Extract named entities (AI-related terms, datasets, models)\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {\"ORG\", \"PRODUCT\", \"EVENT\"}]\n",
    "    return list(set(entities))\n",
    "\n",
    "# Provide the path to your PDFs folder\n",
    "pdf_folder_path = \"pdfs\"  # Update this to your actual folder path\n",
    "\n",
    "# Extract and process text\n",
    "raw_texts = extract_text_from_pdfs(pdf_folder_path)\n",
    "processed_texts = [preprocess_text(text) for text in raw_texts]\n",
    "\n",
    "# Get key terms\n",
    "tfidf_keywords = extract_keywords_tfidf(processed_texts)\n",
    "named_entities = []\n",
    "for text in raw_texts:\n",
    "    named_entities.extend(extract_named_entities(text))\n",
    "\n",
    "# Print extracted key concepts\n",
    "print(\"üîπ Extracted Key Concepts:\")\n",
    "print(\"üìå Top Keywords:\", tfidf_keywords)\n",
    "print(\"üìå Named Entities:\", list(set(named_entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing Classes: {'figure', 'sampling', 'uncertainty', 'model', 'available', 'cid', 'design', 'human', 'agent', 'ai', 'cluster', 'wtchangetalhumancenteredaifordementiacare', 'al', 'acc', 'user', 'trust', 'pvossenetalaconversationalagent', 'rl', 'learning', 'data'}\n",
      "‚ùå Missing Object Properties: {'figure', 'sampling', 'uncertainty', 'model', 'available', 'cid', 'design', 'human', 'agent', 'ai', 'cluster', 'wtchangetalhumancenteredaifordementiacare', 'al', 'acc', 'user', 'trust', 'pvossenetalaconversationalagent', 'rl', 'learning', 'data'}\n",
      "‚ùå Missing Data Properties: {'figure', 'sampling', 'uncertainty', 'model', 'available', 'cid', 'design', 'human', 'agent', 'ai', 'cluster', 'wtchangetalhumancenteredaifordementiacare', 'al', 'acc', 'user', 'trust', 'pvossenetalaconversationalagent', 'rl', 'learning', 'data'}\n"
     ]
    }
   ],
   "source": [
    "# Convert extracted keywords to a set for comparison\n",
    "extracted_keywords_set = set(tfidf_keywords)\n",
    "extracted_named_entities_set = set(named_entities)\n",
    "\n",
    "# Convert existing ontology classes and properties into sets\n",
    "existing_classes_set = set([s.split(\"#\")[-1] for s in existing_classes])\n",
    "existing_object_properties_set = set([s.split(\"#\")[-1] for s in existing_object_properties])\n",
    "existing_data_properties_set = set([s.split(\"#\")[-1] for s in existing_data_properties])\n",
    "\n",
    "# Find missing classes\n",
    "missing_classes = extracted_keywords_set - existing_classes_set\n",
    "\n",
    "# Find missing object properties\n",
    "missing_object_properties = extracted_keywords_set - existing_object_properties_set\n",
    "\n",
    "# Find missing data properties (assuming no existing ones)\n",
    "missing_data_properties = extracted_keywords_set - existing_data_properties_set if existing_data_properties else extracted_keywords_set\n",
    "\n",
    "# Print results\n",
    "print(\"‚ùå Missing Classes:\", missing_classes)\n",
    "print(\"‚ùå Missing Object Properties:\", missing_object_properties)\n",
    "print(\"‚ùå Missing Data Properties:\", missing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated successfully! Saved as updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "from rdflib import URIRef, Graph, Namespace, RDF, RDFS, OWL, XSD, Literal\n",
    "\n",
    "# Define the namespace (update it based on your ontology's namespace)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Function to clean concept names (replace spaces and special chars, ensure valid URI)\n",
    "def clean_name(name):\n",
    "    return urllib.parse.quote(name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\"))\n",
    "\n",
    "# Define safe mappings for problematic words\n",
    "safe_mappings = {\n",
    "    \"acc\": \"Accuracy\",\n",
    "    \"ai\": \"Artificial_Intelligence\",\n",
    "    \"rl\": \"Reinforcement_Learning\",\n",
    "    \"cid\": \"Concept_ID\",\n",
    "    \"al\": \"Algorithm\",\n",
    "    \"pvossenetalaconversationalagent\": \"Conversational_Agent\",\n",
    "    \"wtchangetalhumancenteredaifordementiacare\": \"Human_Centered_AI_Dementia\",\n",
    "}\n",
    "\n",
    "# Function to get safe name\n",
    "def get_safe_name(name):\n",
    "    return safe_mappings.get(name, clean_name(name))\n",
    "\n",
    "# Add missing classes\n",
    "for class_name in missing_classes:\n",
    "    class_name_clean = get_safe_name(class_name)\n",
    "    class_uri = HI[class_name_clean]\n",
    "    g.add((class_uri, RDF.type, OWL.Class))\n",
    "    g.add((class_uri, RDFS.label, Literal(class_name_clean)))  \n",
    "    g.add((class_uri, RDFS.comment, Literal(f\"Automatically added class: {class_name_clean}\")))  \n",
    "\n",
    "# Add missing object properties\n",
    "for prop in missing_object_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, HI[\"AIConcept\"]))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added object property: {prop_clean}\")))\n",
    "\n",
    "# Add missing data properties\n",
    "for prop in missing_data_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, XSD.string))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added data property: {prop_clean}\")))  \n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated successfully! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_titles=[\n",
    "    \"On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making\",\n",
    "\"Value-aware active learning\",\n",
    "\"Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios\",\n",
    "\"Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches\",\n",
    "\"Trust in Clinical AI: Expanding the Unit of Analysis\",\n",
    "\"A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools\",\n",
    "\"Exploring the Dynamic Nature of Trust Using Interventions in a Human-AI Collaborative Task\",\n",
    "\"A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning & Well-being\", \n",
    "\"Knowledge Graphs in Support of Human-Machine Intelligence\",\n",
    "\"Exosoul: ethical profiling in the digital world\",\n",
    "\"Landmarks in Case-based Reasoning: From Theory to Data\",\n",
    "\"Validation of a Measure of Trust in Artificial Intelligence\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making (2023)\n",
      "   üìù Authors: Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, Gerhard Satzger\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230074\n",
      "   üåê URL: https://doi.org/10.3233/faia230074\n",
      "\n",
      "üìå Value-Aware Active Learning (2023)\n",
      "   üìù Authors: Burcu Sayin, Jie Yang, Andrea Passerini, Fabio Casati\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230085\n",
      "   üåê URL: https://doi.org/10.3233/faia230085\n",
      "\n",
      "üìå Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios (2024)\n",
      "   üìù Authors: Wen-Tseng Chang, Shihan Wang, Stephanie Kramer, Michel Oey, Somaya Ben Allouch\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240185\n",
      "   üåê URL: https://doi.org/10.3233/faia240185\n",
      "\n",
      "üìå Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches (2023)\n",
      "   üìù Authors: Matthew E. Taylor\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230098\n",
      "   üåê URL: https://doi.org/10.3233/faia230098\n",
      "\n",
      "üìå Trust in Clinical AI: Expanding the Unit of Analysis (2022)\n",
      "   üìù Authors: Jacob T. Browne, Saskia Bakker, Bin Yu, Peter Lloyd, Somaya Ben Allouch\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220192\n",
      "   üåê URL: https://doi.org/10.3233/faia220192\n",
      "\n",
      "üìå A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools (2023)\n",
      "   üìù Authors: Yaoli Mao, Janet Rafner, Yi Wang, Jacob Sherson\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230078\n",
      "   üåê URL: https://doi.org/10.3233/faia230078\n",
      "\n",
      "üìå Landmarks in Case-Based Reasoning: From Theory to Data (2022)\n",
      "   üìù Authors: Wijnand van Woerkom, Davide Grossi, Henry Prakken, Bart Verheij\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220200\n",
      "   üåê URL: https://doi.org/10.3233/faia220200\n",
      "\n",
      "üìå A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning &amp; Well-Being (2024)\n",
      "   üìù Authors: Piek Vossen, Selene B√°ez Santamar√≠a, Thomas Baier\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240204\n",
      "   üåê URL: https://doi.org/10.3233/faia240204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_metadata_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url).json()\n",
    "    if \"message\" in response:\n",
    "        metadata = response[\"message\"]\n",
    "        return {\n",
    "            \"title\": metadata.get(\"title\", [\"Unknown\"])[0],\n",
    "            \"authors\": \", \".join([author.get(\"given\", \"\") + \" \" + author.get(\"family\", \"\") \n",
    "                                  for author in metadata.get(\"author\", [])]),\n",
    "            \"year\": metadata.get(\"published-print\", {}).get(\"date-parts\", [[None]])[0][0],\n",
    "            \"journal\": metadata.get(\"container-title\", [\"Unknown\"])[0],\n",
    "            \"doi\": metadata.get(\"DOI\", \"\"),\n",
    "            \"url\": metadata.get(\"URL\", \"\")\n",
    "        }\n",
    "    return None\n",
    "\n",
    "doi_list = [\"10.3233/FAIA230074\",\n",
    "            \"10.3233/FAIA230085\",\n",
    "            \"10.3233/FAIA240185\",\n",
    "            \"10.3233/FAIA230098\",\n",
    "            \"10.3233/FAIA220192\",\n",
    "            \"10.3233/FAIA230078\",\n",
    "            \"10.3233/FAIA220200\",\n",
    "            \"10.3233/FAIA240204\"]  # Replace with your DOIs\n",
    "papers_metadata = [get_metadata_from_doi(doi) for doi in doi_list]\n",
    "\n",
    "# Print results\n",
    "for paper in papers_metadata:\n",
    "    print(f\"üìå {paper['title']} ({paper['year']})\")\n",
    "    print(f\"   üìù Authors: {paper['authors']}\")\n",
    "    print(f\"   üìñ Journal: {paper['journal']}\")\n",
    "    print(f\"   üîó DOI: {paper['doi']}\")\n",
    "    print(f\"   üåê URL: {paper['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error fetching metadata for: Value-aware active learning\n",
      "‚ùå Error fetching metadata for: Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios\n",
      "‚ùå Error fetching metadata for: Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches\n",
      "‚ùå Error fetching metadata for: Trust in Clinical AI: Expanding the Unit of Analysis\n",
      "‚ùå Error fetching metadata for: A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools\n",
      "‚ùå Error fetching metadata for: Exploring the Dynamic Nature of Trust Using Interventions in a Human-AI Collaborative Task\n",
      "‚ùå Error fetching metadata for: A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning & Well-being\n",
      "‚ùå Error fetching metadata for: Knowledge Graphs in Support of Human-Machine Intelligence\n",
      "‚ùå Error fetching metadata for: Exosoul: ethical profiling in the digital world\n",
      "‚ùå Error fetching metadata for: Landmarks in Case-based Reasoning: From Theory to Data\n",
      "‚ùå Error fetching metadata for: Validation of a Measure of Trust in Artificial Intelligence\n",
      "üìÑ Research Papers Metadata:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "# Semantic Scholar API endpoint\n",
    "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "# Function to fetch metadata\n",
    "def get_paper_metadata(title):\n",
    "    params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"title,authors,year,doi,venue,abstract\"\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "            paper = data[\"data\"][0]  # Take the first matching result\n",
    "            return {\n",
    "                \"title\": paper.get(\"title\", \"N/A\"),\n",
    "                \"authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])]),\n",
    "                \"year\": paper.get(\"year\", \"N/A\"),\n",
    "                \"doi\": paper.get(\"doi\", \"N/A\"),\n",
    "                \"venue\": paper.get(\"venue\", \"N/A\"),\n",
    "                \"abstract\": paper.get(\"abstract\", \"N/A\")\n",
    "            }\n",
    "    else:\n",
    "        print(f\"‚ùå Error fetching metadata for: {title}\")\n",
    "    return None\n",
    "\n",
    "# Fetch metadata for all papers\n",
    "papers_metadata = []\n",
    "for title in extracted_titles:\n",
    "    metadata = get_paper_metadata(title)\n",
    "    if metadata:\n",
    "        papers_metadata.append(metadata)\n",
    "    time.sleep(1)  # Respect API rate limits\n",
    "\n",
    "# Print the structured list\n",
    "print(\"üìÑ Research Papers Metadata:\")\n",
    "for paper in papers_metadata:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'related_concepts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m g\u001b[38;5;241m.\u001b[39madd((paper_uri, hasDOI, Literal(paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Link paper to concepts\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelated_concepts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     55\u001b[0m     concept_uri \u001b[38;5;241m=\u001b[39m HI[concept\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     56\u001b[0m     g\u001b[38;5;241m.\u001b[39madd((paper_uri, hasConcept, concept_uri))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'related_concepts'"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, XSD, Literal, URIRef\n",
    "\n",
    "# Define the namespace (Ensure it matches your ontology)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new class for research papers\n",
    "ResearchPaper = HI[\"ResearchPaper\"]\n",
    "g.add((ResearchPaper, RDF.type, OWL.Class))\n",
    "g.add((ResearchPaper, RDFS.label, Literal(\"Research Paper\")))\n",
    "g.add((ResearchPaper, RDFS.comment, Literal(\"Class representing research papers in the ontology.\")))\n",
    "\n",
    "# Sample extracted key concepts (from TF-IDF + Named Entities)\n",
    "key_concepts = [\"Trust\", \"AI\", \"Reinforcement Learning\", \"Human-AI Interaction\", \"Uncertainty\", \"Knowledge Graphs\"]\n",
    "\n",
    "# Create instances for concepts (ensure they are part of ontology classes)\n",
    "for concept in key_concepts:\n",
    "    concept_uri = HI[concept.replace(\" \", \"_\")]\n",
    "    g.add((concept_uri, RDF.type, HI[\"Concept\"]))  # Assuming Concept class exists\n",
    "    g.add((concept_uri, RDFS.label, Literal(concept)))\n",
    "    g.add((concept_uri, RDFS.comment, Literal(f\"Automatically added concept: {concept}\")))\n",
    "\n",
    "\n",
    "# Define object properties\n",
    "hasConcept = HI[\"hasConcept\"]\n",
    "g.add((hasConcept, RDF.type, OWL.ObjectProperty))\n",
    "g.add((hasConcept, RDFS.label, Literal(\"hasConcept\")))\n",
    "g.add((hasConcept, RDFS.comment, Literal(\"Links a research paper to a concept\")))\n",
    "\n",
    "# Define data properties\n",
    "hasTitle = HI[\"hasTitle\"]\n",
    "hasAuthor = HI[\"hasAuthor\"]\n",
    "hasYear = HI[\"hasYear\"]\n",
    "hasDOI = HI[\"hasDOI\"]\n",
    "\n",
    "for prop in [hasTitle, hasAuthor, hasYear, hasDOI]:\n",
    "    g.add((prop, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((prop, RDFS.domain, ResearchPaper))\n",
    "    g.add((prop, RDFS.range, XSD.string if prop != hasYear else XSD.integer))\n",
    "\n",
    "# Add instances for research papers\n",
    "for paper in papers_metadata:\n",
    "    paper_uri = HI[paper[\"title\"].replace(\" \", \"_\")]\n",
    "    g.add((paper_uri, RDF.type, ResearchPaper))\n",
    "    g.add((paper_uri, hasTitle, Literal(paper[\"title\"])))\n",
    "    g.add((paper_uri, hasAuthor, Literal(paper[\"authors\"])))\n",
    "    g.add((paper_uri, hasYear, Literal(paper[\"year\"], datatype=XSD.integer)))\n",
    "    g.add((paper_uri, hasDOI, Literal(paper[\"doi\"])))\n",
    "\n",
    "    # Link paper to concepts\n",
    "    for concept in paper[\"related_concepts\"]:\n",
    "        concept_uri = HI[concept.replace(\" \", \"_\")]\n",
    "        g.add((paper_uri, hasConcept, concept_uri))\n",
    "\n",
    "# Save the updated ontology with instances\n",
    "updated_ontology_path = \"updated_hi_ontology_with_instances.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated with RDF instances! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'keywords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m g\u001b[38;5;241m.\u001b[39madd((paper_uri, HI[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhasDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m], Literal(paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Link paper to concepts\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     52\u001b[0m     concept_uri \u001b[38;5;241m=\u001b[39m HI[concept\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     53\u001b[0m     g\u001b[38;5;241m.\u001b[39madd((paper_uri, hasConcept, concept_uri))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'keywords'"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, XSD, Literal, URIRef\n",
    "\n",
    "# Define the namespace (Ensure it matches your ontology)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define ResearchPaper class if not already defined\n",
    "ResearchPaper = HI[\"ResearchPaper\"]\n",
    "g.add((ResearchPaper, RDF.type, OWL.Class))\n",
    "g.add((ResearchPaper, RDFS.label, Literal(\"Research Paper\")))\n",
    "g.add((ResearchPaper, RDFS.comment, Literal(\"Class representing research papers in the ontology.\")))\n",
    "\n",
    "\n",
    "# Define new data properties\n",
    "hasAbstract = HI[\"hasAbstract\"]\n",
    "hasVenue = HI[\"hasVenue\"]\n",
    "hasCitations = HI[\"hasCitations\"]\n",
    "hasKeywords = HI[\"hasKeywords\"]\n",
    "hasExternalLink = HI[\"hasExternalLink\"]\n",
    "\n",
    "# Add data properties to ontology\n",
    "for prop in [hasAbstract, hasVenue, hasKeywords, hasExternalLink]:\n",
    "    g.add((prop, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((prop, RDFS.domain, ResearchPaper))\n",
    "    g.add((prop, RDFS.range, XSD.string))\n",
    "\n",
    "# Add citations as an integer property\n",
    "g.add((hasCitations, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((hasCitations, RDFS.domain, ResearchPaper))\n",
    "g.add((hasCitations, RDFS.range, XSD.integer))\n",
    "\n",
    "# Define object properties\n",
    "hasConcept = HI[\"hasConcept\"]\n",
    "g.add((hasConcept, RDF.type, OWL.ObjectProperty))\n",
    "g.add((hasConcept, RDFS.label, Literal(\"hasConcept\")))\n",
    "g.add((hasConcept, RDFS.comment, Literal(\"Links a research paper to a concept\")))\n",
    "\n",
    "# Add instances for research papers\n",
    "for paper in papers_metadata:\n",
    "    paper_uri = HI[paper[\"title\"].replace(\" \", \"_\")]\n",
    "    g.add((paper_uri, RDF.type, ResearchPaper))\n",
    "    g.add((paper_uri, HI[\"hasTitle\"], Literal(paper[\"title\"])))\n",
    "    g.add((paper_uri, HI[\"hasAuthor\"], Literal(paper[\"authors\"])))\n",
    "    g.add((paper_uri, HI[\"hasYear\"], Literal(paper[\"year\"], datatype=XSD.integer)))\n",
    "    g.add((paper_uri, HI[\"hasDOI\"], Literal(paper[\"doi\"])))\n",
    "    g.add((paper_uri, hasCitations, Literal(paper[\"citations\"], datatype=XSD.integer)))\n",
    "    g.add((paper_uri, hasKeywords, Literal(\", \".join(paper[\"keywords\"]))))\n",
    "    g.add((paper_uri, hasExternalLink, Literal(paper[\"external_link\"])))\n",
    "\n",
    "    # Link paper to concepts\n",
    "    for concept in paper[\"keywords\"]:\n",
    "        concept_uri = HI[concept.replace(\" \", \"_\")]\n",
    "        g.add((paper_uri, hasConcept, concept_uri))\n",
    "\n",
    "# Save the updated ontology with enriched metadata\n",
    "updated_ontology_path = \"updated_hi_ontology_with_metadata.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated with enriched metadata! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evoman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
