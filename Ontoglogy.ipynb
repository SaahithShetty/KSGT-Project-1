{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the project shared in github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (7.1.3)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (0.11.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (3.1.4)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install rdflib nltk pdfplumber scikit-learn spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Existing Classes: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/EthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InformationProcessing', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Interaction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Scenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Capability']\n",
      "‚úÖ Existing Object Properties: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/capability', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasEthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasInteraction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/inScenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/informationMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactingAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingInformation', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingTask']\n",
      "‚úÖ Existing Data Properties: []\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, OWL, RDFS\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"  # Update with the correct path\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "\n",
    "# Extract classes\n",
    "existing_classes = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.Class)]\n",
    "\n",
    "# Extract object properties\n",
    "existing_object_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "# Extract data properties\n",
    "existing_data_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "# Print summary\n",
    "print(\"‚úÖ Existing Classes:\", existing_classes)\n",
    "print(\"‚úÖ Existing Object Properties:\", existing_object_properties)\n",
    "print(\"‚úÖ Existing Data Properties:\", existing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Extracted Key Concepts:\n",
      "üìå Top Keywords: ['trust', 'ai', 'agent', 'rl', 'design', 'learning', 'human', 'acc', 'user', 'pvossenetalaconversationalagent', 'sampling', 'figure', 'cid', 'available', 'al', 'data', 'model', 'cluster', 'uncertainty', 'wtchangetalhumancenteredaifordementiacare']\n",
      "üìå Named Entities: ['YunfengZhang', 'Official Journal', 'Parker W', 'Hatice Ferhan Odabasi', 'al', 'Fitzpatrick', 'ElianeSommerfeld', 'Current Psychology', 'Instagram', 'the Clinical AI Deployment Cycle', 'User Modeling', 'JoshuaDMiller', 'AssociationforComputing', 'DZ', 'CRC Press', 'p<.001‚àó‚àó‚àó)andAIoutputtrust(r=.76', 'AI\\nGeometrically', 'Antaki JF', 'Robben S', 'Alessandro Acquisti', 'the EFMI Working Group for Assessment of Health Information Systems', 'SocialScience', 'Acosta-ZazuetaG.A3Dshapegenerativemethodforaestheticprod-', '.315 F', 'Yearb\\nMed Inform', 'Fig.1', 'Feedback Interface', 'P7', 'Ingeneral', 'Agrawal M', '0.392', 'AakritiKumar', 'Thelocationmapislaidonagrid', 'SantaClara', '‚Ä¢ Itisverycapableofperformingitsjob', 'Trust Through Public Accountability', 'Bart', '323', 'Forthisreason', 'n‚ààN', 'GD', 'PadhraicSmyth', 'El≈ºbieta Sanecka', 'PimentelJ', 'http://dx.doi.org/10.1145/3181671', 'Khodyakov D. Trust', 'Ernest H O‚ÄôBoyle', 'LucianoFloridi', 'Holloway A', 'Incontrast', 'Acc', 'UsingaKruskal-Wallisranksum', 'ambi-\\nent', 'Deep Epsilon-Greedy', 'Computing Systems', 'Ely JW', 'LiL', 'Horizon 2020', 'Barda AJ', 'RLOps', 'Ourcontributiontohuman-AIinteractionisthree-fold', '0.443 0.078 0.071', 'Personality,43(5):747‚Äì754,2009', 'LiuY', 'Analyse', 'wewouldexpectthehumantoadheretoA%ofcorrectAIrecommendationsandA%', 'MieleD', '0.641 0.456 0.237', 'Hybrid Human AI Systems', '0.002 0.0', 'n‚Üí‚àû', 'AI\\nW', 'Wilson S, Stumpf S. User', 'Koenigsberg MR', 'InInternational Conference', 'Correctional Offender Management Profiling', 'Kramer RM', 'AarhusUniversity\\nAbstract', 'TOS', 'Costanza', 'means‚Äô[10,15', 'Logistic Regression', 'CBR', 'the 2020 Conference on\\nFairness, Accountability', 'UnitedKingdom', 'MichaelDesmond', 'Ferna¬¥ndez-LealA.Human-', 'CarinaPaine', 'HRI', 'LiY', 'The Pearson Correlation', 'Benda', 'TimeCollaboration', 'SPIE', '0.079 0.034 0.971', 'ETP', 'Kerstholt JH', 'EvitaMarch', 'Anova', 'AutonomousSystems', 'doi.org/10.1109/IROS.2017.8202133', '‚Ä¢ Certainty', 'CTSdynamicallyadaptsitsapproach', 'EHR', 'SomayaBENALLOUCH', 'Delroy L Paulhus', 'User Trust', 'Delft University of Technology', 'the Google Maps-assisted', 'Remark 4.1', 'Riener A', '0.002 0.173 0.0 0.0 0.0', 'University of Amsterdam', 'Nat Med', 'Reliance', 'Clinical AI', 'Social Psychology Bulletin,12(1):75‚Äì80,1986', 'Grasshopper', 'Label 0', 'Hexaco', '0.74', 'NgAY', 'MoharebM', 'SunY', 'DoEYL', 'Povykalo A', 'Pearson', 'Keywords', 'the Netherlands\\nAbstract', 'BMJ Health Care Inform', 'Adaptation and Personalization', 'De Keizer NF', 'QualitativeCo-Occurrences', 'Niklas', 'F', 'Bedoya AD', 'PaolaInverardi', 'ChargeDegree F M\\nForced', 'ProceedingsoftheACM\\nonInteractive', 'Accfinal', 'the 21st International Conference on Intelligent User Interfaces', 'Amsterdam University of Applied Sciences', 'Complianceisoftenusedasabehaviouraldemonstrationoftrustinhuman', 'AntinoKim', 'andContextualThompsonSampling(CTS)[24].Amongthem', 'x‚àó=argmin x‚ààD', 'Larus-Stone N', 'cid:3', 'InthiscontextthemultidisciplinaryprojectExosoul', 'Cabitza F', 'Inference Engine\\nFill', 'ML', 'Sepsis Watch', 'ChuW', 'Certainty', 'Trust Through the Training of the AI', 'Computing Sciences', 'Functioning & Well-Being\\nPiekVOSSENa,1,SeleneBA¬¥EZSANTAMAR¬¥IAaandThomasBAIERa', 'Ahuman', 'Florence L Geis', 'the AAAI Conference on Human Computation\\n', 'Keshmiri S', '0.04', 'SME', 'ACM', 'Keebler JR', 'International Journal', 'WieseE', 'Santos-Rodr¬¥ƒ±guezR', 'Fahim MAA', 'NWO', 'PatrizioMigliarini', 'Tejeda', 'the COMPAS Recidivism Algorithm', 'l', 'Admissions', 'CSCW', 'hasPlace', 'LeeJD.ModelingGoalAlignmentinHuman-AITeaming', 'ACMTransactions', 'Strohm', 'Gutzwiller RS', 'BTOS', 'Smith-Renner A', 'TREWS', 'Nijman SWJ', 'Working Group', 'McDaniel', 'Dementia', 'Ward', 'Novak LL', 'MDP', 'F ‚â∫ M', 'AcademicPress;2021.p.143', 'Functioning', 'Black AW', 'Exosoul', 'Graber ML', 'Wealsosimulatedcriticaleventsduringtherouteplanningtask', '‚é™‚é™‚é®(A correct+O correct)‚àí(100%‚àíAcc AI‚àíA', 'F :s', 'Narcissism', 'W.-T.Changetal./Human', 'therecentlylaunchedChatGPTseemstoprovidehuman', 'Personal Differences', 'Trust Through a Clinical Champion\\nClinical', 'Procedia', 'AIsystemandemphasizetheimportanceofmaintaininguserengagementandreducing', 'Kiesler S', 'KimJ', 'RobinsonL.Personalisation', 'Burcu Sayin', 'ChargeDegree Indicates', 'Trust Through Clinical Trials and Peer Reviewed Publications\\nPrior', 'Florence, Italy: Association for Computational Linguistics', '0.031 0.777', 'People Business SME DevOps Laypeople ML/RL', 'ChristinaGeary', 'Theparticipantcouldinter-\\nvenebyclickingtheInterruptbutton', 'IBM', 'Black-Box Systems', 'European Commission', 'Academy of Management Journal', 'Corollary 5', 'Acc ‚àíA >0.Theminimumachievabledecision', 'Linear Upper Confidence Bound', 'GrainedEmotions', 'BinLiu', 'B√ºy√ºkt√ºr AG', 'the Small COMPAS Data', 'PatrizioPelliccione', 'Business Ethics', 'Horvat CM', 'The European Parliament', 'Fussell SR', 'Legal Theory', 'Digital', 'F :', 'McGraw-Hill', 'ArtificialIntelligence.1999;112(1):181-211', 'CÀáyras K', '0.51 0.493 0.198', 'Journal of Research', 'Defining Trust', 'Argument & Computation', 'Pre-Development', 'CIFARAIChair', 'weuseencoderLLMsfine', 'ArturNilsson', 'Department of Information', 'LLM', 'the Certifiably Optimal Rule Lists', 'Conscientiousness', 'NiklasKuehl', 'a Philips Experience Design', 'the Creative Commons Attribution Non-Commercial License', 'Individual Differences,100:85‚Äì94,2016', 'the Council of the European Union', 'Harv JL & Tech', 'forDET', 'Open Access', 'Monitoringpatientswell', 'Value-Aware Active Learning', 'andFlorianSchaub', 'BMC Med Inform Decis\\nMak', 'LauraPetrich', 'Conf\\nProc Ethnogr Prax Ind Conf', 'Post-Deployment\\n3.5.1', 'Hum Factors', 'contrastwiththistraditionalviewofRL', '0.855 0.631 0.51 0.341', 'the Ethics Position Questionnaire', 'IEEE', '0.96', 'GroveAJ', 'Thispaperdiscusses5stepstowardssuccessfulRLdeployment', 'Althoefer\\nK', 'Expanding Trust', 'KeenJ', 'Caetano', 'Cummings ML', 'Dulac-ArnoldG', 'F, Tischler V', 'MadeleineGrunde-McLaughlin', 'Siewiorek DP', 'MaxSchemmer', '.082)abc p<.0001\\nConscientiousness', 'Davidoff S. Trust', 'M', 'Daronnat S', 'Teller Road', 'Radiol Med', 'Veloso M', 'Distributed Dynamic\\nTeam Trust', 'UsingaBelieve-Desire-Intentprotocol[20],theplatformutilizesmodulesthatde-', 'QR-Code', 'Amodei D. Deep Reinforcement', 'Informatics', 'ICCIDS', 'International Journal of Management and Bussiness', 'GuidelinesforHuman-AIInteractionwhichseekstoguidedinteractionovertimebeyond', 'Acc >Acc', 'x‚àó=argmin', 'Workload', 'DiscussionandConclusion\\nInthiswork', 'Aysen Gurcan Namlu', 'JoshAndres', 'EU', 'The Journal of\\nEducation', 'Adams R', 'Phillips EK', 'ConsideringthecognitiveimpairmentofPwD', 'GP', 'MarcoAutili', 'Komorowski M', 'Success', 'Reinforcement Learning', 'JALT Testing & Evaluation SIG Newsletter', 'the Base Samples of Cognos Analytics', 'Frontiers of Theory and Research', 'Hoesterey S', 'Ourcontributiontohuman-AIin-', 'DCODE', 'A European Review', 'the Dutch Ministry of Education', 'EXOSOUL Ethical Engine Scheme', 'CTS', 'Pak R', 'Thiscodecould ‚Ä¢ Idonotfeeloutofcontrolinworkingwithit', 'Machine Learning', 'NONDET', 'ContextualandNon', 'manexpert‚Äôswillingnesstocontributetotheco-creativetoolduringandafterco-creation', 'Reflexive Thematic Analysis', 'Clusteranalysis.newberrypark,1984', 'r(x', 'Race', 'Computing\\nSystems', 'Inthisbalanceddataset', 'CA', 'MMOLAEE', 'Dark Triad', 'DigitalBehaviors', 'JournalofHumanNutritionandDietetics.2022;35(1):165', 'LiuX', 'Thompson WR', 'Curran\\nAssociates', 'ancinghowimpactfulasuccessfulsolutionwillbe', 'MichaelMuller', 'AI@Trento(FBK-Unitn', 'ACM Conference on Computer Supported Cooperative Work\\n', 'TrustinHuman-Robot\\nInteraction', 'Digital Humanism', 'Martic M', 'asdescribedintheAI-TAM', 'IBM SPSS Statistics', 'Netherlands Organisation for Scientific Research', 'Trust Through Clinical Involvement', 'Antecedents of Trust', 'NPD-TAM', 'aUniversit√†', 'NotethatsinceAcc', 'BMC Med Inform Decis Mak', 'Social Computing', 'AI\\nlearning(r=.74', 's(x)‚â§œÑ(œÑ‚àís(x', 'Computer', 'Paul T Costa Jr', 'AishaAkbar', 'the Dark Triad Dirty', 'Leeuwenberg AM', 'CERTAINTY 5', 'NormanSadeh', 'VirtualAssistants-TheApplication\\nBroadly', 'Human Preferences', '0.144 0.777 0.335', 'AI & law', 'the Hybrid Intelligence Center', '0.632 0.551 0.385', 'USA,2020.AssociationforComputingMachinery', 'communication,17(1):1‚Äì18,2011', 'GrossMD', 'LeeS', 'JP', 'AI', 'SchoolofBusinessand', 'Bernardino A', 'Artificial Intelligence - A', 'Armaan A', 'PiersSteel', 'Antony AS', 'cancelevel:‚àó‚àó‚àóp<.001,‚àó‚àóp<.01,‚àóp<.05', 'University of Groningen', '‚Ä¢ (Thenumberofdesigngoals', 'betweenthelandmarks(œá2=125.36', 'F:TrafficPsychologyandBehaviour.2020;73:15-28', 'Trust Through the AI Development Team', 'Singh P, Pomerantz S, Doyle S, Kakarmath S', 'HCOMP', 'SPIE\\nProceedings', 'KellyT', 'Discovery', 'J Am Med Inform Assoc', 'AAAIPress', 'Asourexperimentalevaluationwillshow', 'Building Trust', 'Dimensions, Preferences', 'GreenewaldK', 'HAI', '0.147 0.018 0.795', 'Nishikawa', 'LuJ', 'the ACM on Human-Computer Interaction', 'American Institute of Aeronautics', '0.045 0.02', 'Train/Val/Testsize', 'User Profiling', 'NASA', 'GDPR', 'International\\nJournal of Mental Health', 'DensityandSaturationcriteria Eventsandtheirpropertiescanbenecessary', '0.716 0.0', 'the Interdependence of Reliance\\nBehavior', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 353', 'Hegde N', 'NPJ Digit Med', 'PatrickHemmer', 'TheAcademyof', 'Utrecht University', 'UserBot', 'CTS 1', 'TomBuchanan', 'theparticipantstrustedandusedtheGPSdrivingadvice', 'Categorical Data', 'correlation[48', 'Yearb Med Inform', 'Science of Computer', 'World Health Organization', '0.076', 'Clare AS', 'HelenaVasconcelos', 'AnupamDas', 'TheRoleofTrustin\\nHuman-RobotInteraction', 'EXOSOULisbasedonthenotion', 'F :s.', 'Paul T Costa', 'NewYork', 'Intelligent Systems Technical Conference', 'Informatics Institute', 'Nature Machine Intelligence', 'Wang', 'the Unit of Analysis of Trust', 'A ‚â• Acc', 'DeepEpsilon-Greedy', 'ChengCY', 'AnEarly\\nDesign Framework to Support Concept Creation and Evaluation', 'Dazeley R', 'McNair JB', 'HjorthA', 'AIaccuracy', 'Table 2', 'Computational Models of Argument', 'SC', 'Journal of Machine Learning Research', 'FactBank', 'International Data Privacy Law', 'Alirezaie M', 'Krishnaswamy N', 'MochenYang', 'IOS Press', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 357', 'Wilcox L', 'Automated Systems', '7.00', 'Northpointe Inc.', 'Nuthakki S', 'Cham,2022', 'ADL', 'RL', 'Journal of personality,83(6):644‚Äì664,2015', 'Taya R Cohen', 'International Journal of Human', 'the Netherlands\\nOrganisation for Scientific Research', 'Philosophy & Technology', 'Prokop M', 'HateSpeech', 'AndreaPASSERINIaandFabioCASATIc', 'Initial Ethical\\nInference Engine Profile', 'Acc ‚àà[10%,50%', 'PwD', 'Construct QualitativeCodes QuantitativeMeasures(DataRange', 'CarinaBenz', 'Sutton RS', 'FDA', 'USA\\nAbstract', 'Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence', 'ProPublica', 'the\\nEuropean Union‚Äôs', 'Boehm-DavisDA.EffectsofAgeandCongestionInformationAccuracyofAdvancedTraveler\\nInformationSystemsonUserTrustandCompliance', 'Journal of Applied', 'EUR', 'AdrianFurnham', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 359\\nReferences', 'Business\\n&InformationSystemsEngineering.2016;58:367-', 'CHI Conference on Human Factors', '0.776 0.487', 'Massimiliano', 'Ontological References', 'Cole', 'Grisel', 'BMC Med', 'dictingdigitalbehaviorsindifferentagesamples(e.g', 'AI-TAM', 'Reale C', '‚Ä¢ Ithinkitisuselesstotrainitwithmylikes', 'AungMH', 'N', 'Northepointe, Inc.', 'MartinDegeling', 'Racedimension', 'Autonomous Systems', 'F for s¬Ø', 'U.S. Food and Drug Administration', 'VanDuinC', 'Machine\\nLearning in Python', 'CTSachievedthebestperformanceamongthealgorithms', 'Swift Trust', 'AdrianoSchimmenti', 'object‚Üíspace', 'DET', 'SchapireRE', 'un', 'J Am Med Inform\\nAssoc', 'Ackerman MS', 'AI\\nAcc ‚àà', 'CeliaMoore', 'CHI Conference', 'Softethicsandthegovernanceofthedigital', 'LiH', 'CAD', 'Healthcare', 'Atari', 'andempathicAIsystemthatcan', '‚Ä¢ Wouldyouinvestyourtimeintrainingtouse', 'KlasnjaP', 'Idonothavetheabilitytoadjusttoindividualpreferencesintheway', 'Scheffler', 'Ashktorab Z', 'Multimodal Semantic Representations', 'Marsh S', 'Muir', '‚Ä¢ Q2', 'Artificial Intelligence (AI', 'DST', 'AI AI AI', 'BaselineResults', 'TAM', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches', 'RobertLoftin', 'andneedsofPwD', 'DesigningforHuman-CenteredSystems', 'MDPandtrytolearnahigherperformingpolicy', 'Tukey‚Äôs Post-hoc', 'RahmaniAslM. SketchOpt', 'forthedeterministic(D)andnon', 'Knowledge-Based Systems', 'USA: Association', 'Community', 'totrainwasassociatedwithmorewillingness(r=.72', 'Center for Devices, Radiological Health', 'COMPAS', 'weshowthatthehumanmayneverbeexpectedtocomplementtheAI', 'the 2020 Conference on Fairness, Accountability', 'Wenamethisstrategythreshold-orientedsampling(TOS).Note', 'LimitationsandConclusions\\nInthiswork', 'Sendak', 'CheXplain', 'Cece', 'Pareek A', 'linear', '6.04 7.00', 'Drinking Scenarios\\nWen-TsengCHANGa', 'the University of L‚ÄôAquila', 'ThomasJ', 'ActivitiesofDailyLife(ADL)[1,2,3].Suchmonitoringis', 'CB', 'Harvard\\nData Science Review', 'CHI\\nConference on Human Factors', 'Chernova S', 'European Dementia Monitor', 'Albayram', 'Platt JE', 'JMIR Med Inform', 'Bellamy RKE', 'LiP', 'EPT', 'inwhichthemodelneverabstains(k=0).However', 'Acc <Acc', 'LiK', 'Med Care Res Rev', 'Pringle C, Eleftheriou I', 'Conversationalintent\\nFortheagentunderconsideration', 'BoneXpert', 'Table 4.3', 'Artificial Intelligence, Trust Calibration', 'CaoS', 'NY', 'US Airlines', 'Neural Information Processing Systems', 'StatisticsAnova']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    extracted_texts = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \" \".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                extracted_texts.append(text)\n",
    "    return extracted_texts\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Extract key terms using TF-IDF\n",
    "def extract_keywords_tfidf(texts, num_keywords=20):\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words=\"english\", ngram_range=(1,2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
    "    keyword_scores = dict(zip(feature_names, tfidf_scores))\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [keyword for keyword, _ in sorted_keywords[:num_keywords]]\n",
    "\n",
    "# Extract named entities (AI-related terms, datasets, models)\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {\"ORG\", \"PRODUCT\", \"EVENT\"}]\n",
    "    return list(set(entities))\n",
    "\n",
    "# Provide the path to your PDFs folder\n",
    "pdf_folder_path = \"pdfs\"  # Update this to your actual folder path\n",
    "\n",
    "# Extract and process text\n",
    "raw_texts = extract_text_from_pdfs(pdf_folder_path)\n",
    "processed_texts = [preprocess_text(text) for text in raw_texts]\n",
    "\n",
    "# Get key terms\n",
    "tfidf_keywords = extract_keywords_tfidf(processed_texts)\n",
    "named_entities = []\n",
    "for text in raw_texts:\n",
    "    named_entities.extend(extract_named_entities(text))\n",
    "\n",
    "# Print extracted key concepts\n",
    "print(\"üîπ Extracted Key Concepts:\")\n",
    "print(\"üìå Top Keywords:\", tfidf_keywords)\n",
    "print(\"üìå Named Entities:\", list(set(named_entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing Classes: {'agent', 'sampling', 'al', 'uncertainty', 'data', 'learning', 'cid', 'figure', 'trust', 'human', 'available', 'design', 'acc', 'ai', 'wtchangetalhumancenteredaifordementiacare', 'user', 'model', 'rl', 'pvossenetalaconversationalagent', 'cluster'}\n",
      "‚ùå Missing Object Properties: {'agent', 'sampling', 'al', 'uncertainty', 'data', 'learning', 'cid', 'figure', 'trust', 'human', 'available', 'design', 'acc', 'ai', 'wtchangetalhumancenteredaifordementiacare', 'user', 'model', 'rl', 'pvossenetalaconversationalagent', 'cluster'}\n",
      "‚ùå Missing Data Properties: {'agent', 'sampling', 'al', 'uncertainty', 'data', 'learning', 'cid', 'figure', 'trust', 'human', 'available', 'design', 'acc', 'ai', 'wtchangetalhumancenteredaifordementiacare', 'user', 'model', 'rl', 'pvossenetalaconversationalagent', 'cluster'}\n"
     ]
    }
   ],
   "source": [
    "# Convert extracted keywords to a set for comparison\n",
    "extracted_keywords_set = set(tfidf_keywords)\n",
    "extracted_named_entities_set = set(named_entities)\n",
    "\n",
    "# Convert existing ontology classes and properties into sets\n",
    "existing_classes_set = set([s.split(\"#\")[-1] for s in existing_classes])\n",
    "existing_object_properties_set = set([s.split(\"#\")[-1] for s in existing_object_properties])\n",
    "existing_data_properties_set = set([s.split(\"#\")[-1] for s in existing_data_properties])\n",
    "\n",
    "# Find missing classes\n",
    "missing_classes = extracted_keywords_set - existing_classes_set\n",
    "\n",
    "# Find missing object properties\n",
    "missing_object_properties = extracted_keywords_set - existing_object_properties_set\n",
    "\n",
    "# Find missing data properties (assuming no existing ones)\n",
    "missing_data_properties = extracted_keywords_set - existing_data_properties_set if existing_data_properties else extracted_keywords_set\n",
    "\n",
    "# Print results\n",
    "print(\"‚ùå Missing Classes:\", missing_classes)\n",
    "print(\"‚ùå Missing Object Properties:\", missing_object_properties)\n",
    "print(\"‚ùå Missing Data Properties:\", missing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated successfully! Saved as updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "from rdflib import URIRef, Graph, Namespace, RDF, RDFS, OWL, XSD, Literal\n",
    "\n",
    "# Define the namespace (update it based on your ontology's namespace)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Function to clean concept names (replace spaces and special chars, ensure valid URI)\n",
    "def clean_name(name):\n",
    "    return urllib.parse.quote(name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\"))\n",
    "\n",
    "# Define safe mappings for problematic words\n",
    "safe_mappings = {\n",
    "    \"acc\": \"Accuracy\",\n",
    "    \"ai\": \"Artificial_Intelligence\",\n",
    "    \"rl\": \"Reinforcement_Learning\",\n",
    "    \"cid\": \"Concept_ID\",\n",
    "    \"al\": \"Algorithm\",\n",
    "    \"pvossenetalaconversationalagent\": \"Conversational_Agent\",\n",
    "    \"wtchangetalhumancenteredaifordementiacare\": \"Human_Centered_AI_Dementia\",\n",
    "}\n",
    "\n",
    "# Function to get safe name\n",
    "def get_safe_name(name):\n",
    "    return safe_mappings.get(name, clean_name(name))\n",
    "\n",
    "# Add missing classes\n",
    "for class_name in missing_classes:\n",
    "    class_name_clean = get_safe_name(class_name)\n",
    "    class_uri = HI[class_name_clean]\n",
    "    g.add((class_uri, RDF.type, OWL.Class))\n",
    "    g.add((class_uri, RDFS.label, Literal(class_name_clean)))  \n",
    "    g.add((class_uri, RDFS.comment, Literal(f\"Automatically added class: {class_name_clean}\")))  \n",
    "\n",
    "# Add missing object properties\n",
    "for prop in missing_object_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, HI[\"AIConcept\"]))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added object property: {prop_clean}\")))\n",
    "\n",
    "# Add missing data properties\n",
    "for prop in missing_data_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, XSD.string))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added data property: {prop_clean}\")))  \n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated successfully! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_titles=[\n",
    "    \"On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making\",\n",
    "\"Value-aware active learning\",\n",
    "\"Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios\",\n",
    "\"Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches\",\n",
    "\"Trust in Clinical AI: Expanding the Unit of Analysis\",\n",
    "\"A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools\",\n",
    "\"Exploring the Dynamic Nature of Trust Using Interventions in a Human-AI Collaborative Task\",\n",
    "\"A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning & Well-being\", \n",
    "\"Knowledge Graphs in Support of Human-Machine Intelligence\",\n",
    "\"Exosoul: ethical profiling in the digital world\",\n",
    "\"Landmarks in Case-based Reasoning: From Theory to Data\",\n",
    "\"Validation of a Measure of Trust in Artificial Intelligence\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making (2023)\n",
      "   üìù Authors: Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, Gerhard Satzger\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230074\n",
      "   üåê URL: https://doi.org/10.3233/faia230074\n",
      "\n",
      "üìå Value-Aware Active Learning (2023)\n",
      "   üìù Authors: Burcu Sayin, Jie Yang, Andrea Passerini, Fabio Casati\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230085\n",
      "   üåê URL: https://doi.org/10.3233/faia230085\n",
      "\n",
      "üìå Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios (2024)\n",
      "   üìù Authors: Wen-Tseng Chang, Shihan Wang, Stephanie Kramer, Michel Oey, Somaya Ben Allouch\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240185\n",
      "   üåê URL: https://doi.org/10.3233/faia240185\n",
      "\n",
      "üìå Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches (2023)\n",
      "   üìù Authors: Matthew E. Taylor\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230098\n",
      "   üåê URL: https://doi.org/10.3233/faia230098\n",
      "\n",
      "üìå Trust in Clinical AI: Expanding the Unit of Analysis (2022)\n",
      "   üìù Authors: Jacob T. Browne, Saskia Bakker, Bin Yu, Peter Lloyd, Somaya Ben Allouch\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220192\n",
      "   üåê URL: https://doi.org/10.3233/faia220192\n",
      "\n",
      "üìå A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools (2023)\n",
      "   üìù Authors: Yaoli Mao, Janet Rafner, Yi Wang, Jacob Sherson\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230078\n",
      "   üåê URL: https://doi.org/10.3233/faia230078\n",
      "\n",
      "üìå Landmarks in Case-Based Reasoning: From Theory to Data (2022)\n",
      "   üìù Authors: Wijnand van Woerkom, Davide Grossi, Henry Prakken, Bart Verheij\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220200\n",
      "   üåê URL: https://doi.org/10.3233/faia220200\n",
      "\n",
      "üìå A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning &amp; Well-Being (2024)\n",
      "   üìù Authors: Piek Vossen, Selene B√°ez Santamar√≠a, Thomas Baier\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240204\n",
      "   üåê URL: https://doi.org/10.3233/faia240204\n",
      "\n",
      "üìå Unknown (None)\n",
      "   üìù Authors: \n",
      "   üìñ Journal: Unknown\n",
      "   üîó DOI: \n",
      "   üåê URL: \n",
      "\n",
      "üìå Exemplars and Counterexemplars Explanations for Skin Lesion Classifiers (2022)\n",
      "   üìù Authors: Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220209\n",
      "   üåê URL: https://doi.org/10.3233/faia220209\n",
      "\n",
      "üìå Unreflected Acceptance ‚Äì Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education (2024)\n",
      "   üìù Authors: Lars Krupp, Steffen Steinert, Maximilian Kiefer-Emmanouilidis, Karina E. Avila, Paul Lukowicz, Jochen Kuhn, Stefan K√ºchemann, Jakob Karolus\n",
      "   üìñ Journal: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240195\n",
      "   üåê URL: https://doi.org/10.3233/faia240195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_metadata_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url).json()\n",
    "    if \"message\" in response:\n",
    "        metadata = response[\"message\"]\n",
    "        return {\n",
    "            \"title\": metadata.get(\"title\", [\"Unknown\"])[0],\n",
    "            \"authors\": \", \".join([author.get(\"given\", \"\") + \" \" + author.get(\"family\", \"\") \n",
    "                                  for author in metadata.get(\"author\", [])]),\n",
    "            \"year\": metadata.get(\"published-print\", {}).get(\"date-parts\", [[None]])[0][0],\n",
    "            \"journal\": metadata.get(\"container-title\", [\"Unknown\"])[0],\n",
    "            \"doi\": metadata.get(\"DOI\", \"\"),\n",
    "            \"url\": metadata.get(\"URL\", \"\")\n",
    "        }\n",
    "    return None\n",
    "\n",
    "doi_list = [\"10.3233/FAIA230074\",\n",
    "            \"10.3233/FAIA230085\",\n",
    "            \"10.3233/FAIA240185\",\n",
    "            \"10.3233/FAIA230098\",\n",
    "            \"10.3233/FAIA220192\",\n",
    "            \"10.3233/FAIA230078\",\n",
    "            \"10.3233/FAIA220200\",\n",
    "            \"10.3233/FAIA240204\",\"\",\"10.3233/FAIA220209\",\"10.3233/FAIA240195\"]  # Replace with your DOIs\n",
    "papers_metadata = [get_metadata_from_doi(doi) for doi in doi_list]\n",
    "\n",
    "# Print results\n",
    "for paper in papers_metadata:\n",
    "    print(f\"üìå {paper['title']} ({paper['year']})\")\n",
    "    print(f\"   üìù Authors: {paper['authors']}\")\n",
    "    print(f\"   üìñ Journal: {paper['journal']}\")\n",
    "    print(f\"   üîó DOI: {paper['doi']}\")\n",
    "    print(f\"   üåê URL: {paper['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded existing ontology: updated_hi_ontology.ttl\n",
      "‚úÖ Ontology successfully updated and saved in updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, XSD, Literal, URIRef\n",
    "import urllib.parse\n",
    "\n",
    "# üìå Define the namespace (Ensure it matches your ontology)\n",
    "ontology_path = \"updated_hi_ontology.ttl\"  # Update with your actual ontology file\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# üìå Load existing ontology before modification\n",
    "g = Graph()\n",
    "try:\n",
    "    g.parse(ontology_path, format=\"turtle\")  # Load existing ontology\n",
    "    print(f\"‚úÖ Successfully loaded existing ontology: {ontology_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading ontology: {e}. Creating a new ontology.\")\n",
    "\n",
    "g.bind(\"hi\", HI)  # Ensure the namespace is bound\n",
    "\n",
    "# üìå Define the ResearchPaper class if not already in ontology\n",
    "ResearchPaper = HI[\"ResearchPaper\"]\n",
    "if (ResearchPaper, RDF.type, OWL.Class) not in g:\n",
    "    g.add((ResearchPaper, RDF.type, OWL.Class))\n",
    "    g.add((ResearchPaper, RDFS.label, Literal(\"Research Paper\")))\n",
    "    g.add((ResearchPaper, RDFS.comment, Literal(\"Class representing research papers in the ontology.\")))\n",
    "\n",
    "# üìå Define new data properties if not already in ontology\n",
    "data_properties = {\n",
    "    \"hasAbstract\": XSD.string,\n",
    "    \"hasVenue\": XSD.string,\n",
    "    \"hasKeywords\": XSD.string,\n",
    "    \"hasExternalLink\": XSD.string,\n",
    "    \"hasCitations\": XSD.integer,\n",
    "    \"hasTitle\": XSD.string,\n",
    "    \"hasAuthor\": XSD.string,\n",
    "    \"hasYear\": XSD.integer,\n",
    "    \"hasDOI\": XSD.string\n",
    "}\n",
    "\n",
    "for prop, dtype in data_properties.items():\n",
    "    prop_uri = HI[prop]\n",
    "    if (prop_uri, RDF.type, OWL.DatatypeProperty) not in g:\n",
    "        g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "        g.add((prop_uri, RDFS.domain, ResearchPaper))\n",
    "        g.add((prop_uri, RDFS.range, dtype))\n",
    "        g.add((prop_uri, RDFS.label, Literal(prop)))\n",
    "        g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added data property: {prop}\")))\n",
    "\n",
    "# üìå Define object properties if not already in ontology\n",
    "hasConcept = HI[\"hasConcept\"]\n",
    "if (hasConcept, RDF.type, OWL.ObjectProperty) not in g:\n",
    "    g.add((hasConcept, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((hasConcept, RDFS.label, Literal(\"hasConcept\")))\n",
    "    g.add((hasConcept, RDFS.comment, Literal(\"Links a research paper to a concept\")))\n",
    "\n",
    "# üìå Sample metadata for research papers (Update this list dynamically)\n",
    "papers_metadata = [\n",
    "    {\"title\": \"On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making\",\n",
    "     \"authors\": \"John Doe, Jane Smith\",\n",
    "     \"year\": 2023,\n",
    "     \"doi\": \"10.3233/FAIA230074\",\n",
    "     \"url\": \"https://doi.org/10.3233/FAIA230074\",\n",
    "     \"venue\": \"FAIA\",\n",
    "     \"abstract\": \"This paper explores the interdependence of reliance behavior and accuracy in AI-assisted decision-making.\",\n",
    "     \"keywords\": \"AI, Decision-Making, Trust, Human-AI Interaction\"},\n",
    "    \n",
    "    {\"title\": \"Value-aware active learning\",\n",
    "     \"authors\": \"Alice Brown, Bob White\",\n",
    "     \"year\": 2022,\n",
    "     \"doi\": \"10.3233/FAIA230085\",\n",
    "     \"url\": \"https://doi.org/10.3233/FAIA230085\",\n",
    "     \"venue\": \"FAIA\",\n",
    "     \"abstract\": \"This study introduces a novel approach to active learning that integrates value-awareness for improved model training.\",\n",
    "     \"keywords\": \"Machine Learning, Active Learning, Model Training, Value-aware Learning\"}\n",
    "]\n",
    "\n",
    "# üìå Add instances for research papers (only if not already present)\n",
    "for paper in papers_metadata:\n",
    "    paper_uri = HI[urllib.parse.quote(paper[\"title\"].replace(\" \", \"_\"))]\n",
    "\n",
    "    if (paper_uri, RDF.type, ResearchPaper) not in g:\n",
    "        g.add((paper_uri, RDF.type, ResearchPaper))\n",
    "        g.add((paper_uri, HI[\"hasTitle\"], Literal(paper[\"title\"])))\n",
    "        g.add((paper_uri, HI[\"hasAuthor\"], Literal(paper[\"authors\"])))\n",
    "        g.add((paper_uri, HI[\"hasYear\"], Literal(paper[\"year\"], datatype=XSD.integer)))\n",
    "        g.add((paper_uri, HI[\"hasDOI\"], Literal(paper[\"doi\"])))\n",
    "        g.add((paper_uri, HI[\"hasExternalLink\"], Literal(paper[\"url\"])))\n",
    "        g.add((paper_uri, HI[\"hasVenue\"], Literal(paper[\"venue\"])))\n",
    "        g.add((paper_uri, HI[\"hasAbstract\"], Literal(paper[\"abstract\"])))\n",
    "        g.add((paper_uri, HI[\"hasKeywords\"], Literal(paper[\"keywords\"])))\n",
    "\n",
    "# üìå Save the updated ontology back to the same file\n",
    "g.serialize(destination=ontology_path, format=\"turtle\")\n",
    "print(f\"‚úÖ Ontology successfully updated and saved in {ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evoman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
