{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (7.1.3)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (0.11.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rdflib) (3.1.4)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/evoman/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install rdflib nltk pdfplumber scikit-learn spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Existing Classes: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/EthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InformationProcessing', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Interaction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Scenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Capability']\n",
      "‚úÖ Existing Object Properties: ['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/capability', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasEthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasInteraction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/inScenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/informationMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactingAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingInformation', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingTask']\n",
      "‚úÖ Existing Data Properties: []\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, OWL, RDFS\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"  # Update with the correct path\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "\n",
    "# Extract classes\n",
    "existing_classes = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.Class)]\n",
    "\n",
    "# Extract object properties\n",
    "existing_object_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "# Extract data properties\n",
    "existing_data_properties = [str(s.split(\"#\")[-1]) for s in g.subjects(RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "# Print summary\n",
    "print(\"‚úÖ Existing Classes:\", existing_classes)\n",
    "print(\"‚úÖ Existing Object Properties:\", existing_object_properties)\n",
    "print(\"‚úÖ Existing Data Properties:\", existing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saahithshetty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Extracted Key Concepts:\n",
      "üìå Top Keywords: ['trust', 'ai', 'agent', 'rl', 'design', 'learning', 'human', 'acc', 'user', 'pvossenetalaconversationalagent', 'sampling', 'figure', 'cid', 'available', 'al', 'data', 'model', 'cluster', 'uncertainty', 'wtchangetalhumancenteredaifordementiacare']\n",
      "üìå Named Entities: ['Individual Differences,100:85‚Äì94,2016', 'forDET', 'Caetano', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 353', 'the Certifiably Optimal Rule Lists', 'Acc', 'Harv JL & Tech', 'Monitoringpatientswell', 'AI\\nW', 'GreenewaldK', 'Taya R Cohen', 'Singh P, Pomerantz S, Doyle S, Kakarmath S', 'Digital Humanism', '0.002 0.0', 'Antecedents of Trust', 'Idonothavetheabilitytoadjusttoindividualpreferencesintheway', 'AIsystemandemphasizetheimportanceofmaintaininguserengagementandreducing', 'Hegde N', 'International Journal of Human', 'ACM Conference on Computer Supported Cooperative Work\\n', 'Artificial Intelligence (AI', 'HCOMP', '0.04', 'LimitationsandConclusions\\nInthiswork', 'r(x', 'Ernest H O‚ÄôBoyle', 'andFlorianSchaub', 'FDA', 'JournalofHumanNutritionandDietetics.2022;35(1):165', 'PimentelJ', 'Parker W', 'dictingdigitalbehaviorsindifferentagesamples(e.g', 'Reliance', 'RobinsonL.Personalisation', 'contrastwiththistraditionalviewofRL', 'the Netherlands\\nOrganisation for Scientific Research', 'Tukey‚Äôs Post-hoc', 'F :s', 'NONDET', 'Boehm-DavisDA.EffectsofAgeandCongestionInformationAccuracyofAdvancedTraveler\\nInformationSystemsonUserTrustandCompliance', 'Philosophy & Technology', 'Ahuman', 'TheAcademyof', 'correlation[48', 'JoshuaDMiller', 'PiersSteel', 'the ACM on Human-Computer Interaction', 'DET', 'Florence L Geis', 'LiK', 'NPD-TAM', 'ProceedingsoftheACM\\nonInteractive', 'MartinDegeling', 'Race', 'Cham,2022', 'B√ºy√ºkt√ºr AG', 'cid:3', 'the Hybrid Intelligence Center', 'Wealsosimulatedcriticaleventsduringtherouteplanningtask', 'DesigningforHuman-CenteredSystems', 'SPIE', 'AI\\nlearning(r=.74', 'CAD', 'Computing Sciences', 'Barda AJ', 'Legal Theory', 'forthedeterministic(D)andnon', 'PatrickHemmer', 'Curran\\nAssociates', 'Journal of personality,83(6):644‚Äì664,2015', 'Adams R', 'Expanding Trust', 'European Commission', 'ETP', 'Platt JE', 'Incontrast', 'CIFARAIChair', 'betweenthelandmarks(œá2=125.36', '0.51 0.493 0.198', 'Inthisbalanceddataset', 'JP', 'F :', 'MieleD', 'M', 'International Journal', 'CÀáyras K', 'ChengCY', 'The Pearson Correlation', 'LuJ', 'n‚ààN', 's(x)‚â§œÑ(œÑ‚àís(x', 'Burcu Sayin', 'User Profiling', 'theparticipantstrustedandusedtheGPSdrivingadvice', 'Neural Information Processing Systems', 'MarcoAutili', 'linear', 'Trust Through a Clinical Champion\\nClinical', 'El≈ºbieta Sanecka', 'Harvard\\nData Science Review', '0.392', 'http://dx.doi.org/10.1145/3181671', 'SomayaBENALLOUCH', 'CTSachievedthebestperformanceamongthealgorithms', 'andempathicAIsystemthatcan', 'Bernardino A', 'the\\nEuropean Union‚Äôs', 'Black-Box Systems', 'Graber ML', 'ChargeDegree Indicates', 'P7', 'QualitativeCo-Occurrences', 'Alirezaie M', 'LiY', '0.716 0.0', 'Horvat CM', 'Grisel', 'Drinking Scenarios\\nWen-TsengCHANGa', 'J Am Med Inform Assoc', 'HRI', 'DZ', '0.632 0.551 0.385', 'inwhichthemodelneverabstains(k=0).However', 'the AAAI Conference on Human Computation\\n', 'Reale C', 'Khodyakov D. Trust', 'Leeuwenberg AM', 'Swift Trust', 'Journal of Applied', 'Certainty', 'Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence', 'ambi-\\nent', 'PatrizioMigliarini', 'ContextualandNon', 'ChuW', 'TimeCollaboration', 'Personality,43(5):747‚Äì754,2009', 'BMJ Health Care Inform', 'People Business SME DevOps Laypeople ML/RL', 'Analyse', 'Alessandro Acquisti', 'Benda', 'totrainwasassociatedwithmorewillingness(r=.72', 'University of Amsterdam', '‚Ä¢ Ithinkitisuselesstotrainitwithmylikes', 'HAI', 'asdescribedintheAI-TAM', 'W.-T.Changetal./Human', 'McNair JB', 'Holloway A', 'AI & law', 'The European Parliament', 'IBM', 'Official Journal', 'User Trust', 'Admissions', 'Functioning & Well-Being\\nPiekVOSSENa,1,SeleneBA¬¥EZSANTAMAR¬¥IAaandThomasBAIERa', '0.147 0.018 0.795', 'the Base Samples of Cognos Analytics', 'Ontological References', 'Center for Devices, Radiological Health', 'Teller Road', 'WieseE', '0.641 0.456 0.237', 'NiklasKuehl', 'Nijman SWJ', 'the Dutch Ministry of Education', 'Komorowski M', 'HjorthA', 'NotethatsinceAcc', 'Academy of Management Journal', 'Bedoya AD', 'DoEYL', 'Santos-Rodr¬¥ƒ±guezR', 'ChargeDegree F M\\nForced', 'Fahim MAA', 'MichaelMuller', 'Black AW', 'CarinaBenz', 'Human Preferences', 'NPJ Digit Med', 'RL', 'Asourexperimentalevaluationwillshow', 'Albayram', 'Informatics Institute', 'LiuY', 'Instagram', 'GrossMD', 'Train/Val/Testsize', 'weshowthatthehumanmayneverbeexpectedtocomplementtheAI', 'AssociationforComputing', 'MadeleineGrunde-McLaughlin', 'Antaki JF', 'Procedia', 'UsingaBelieve-Desire-Intentprotocol[20],theplatformutilizesmodulesthatde-', 'Muir', 'Nuthakki S', 'Conversationalintent\\nFortheagentunderconsideration', 'Keshmiri S', 'the Ethics Position Questionnaire', 'UsingaKruskal-Wallisranksum', 'Sutton RS', 'Paul T Costa Jr', 'FactBank', 'Logistic Regression', 'Pringle C, Eleftheriou I', 'Reflexive Thematic Analysis', '0.443 0.078 0.071', 'TrustinHuman-Robot\\nInteraction', 'Med Care Res Rev', 'World Health Organization', '0.031 0.777', 'Wilson S, Stumpf S. User', 'the University of L‚ÄôAquila', 'AnEarly\\nDesign Framework to Support Concept Creation and Evaluation', 'SchapireRE', 'Amsterdam University of Applied Sciences', 'Nature Machine Intelligence', 'Antony AS', 'GD', 'Ward', 'AdrianFurnham', 'Prokop M', 'GDPR', 'Hybrid Human AI Systems', '0.076', 'Open Access', 'Ourcontributiontohuman-AIinteractionisthree-fold', 'ArturNilsson', 'Hoesterey S', 'the Council of the European Union', 'Conscientiousness', 'KimJ', 'Nat Med', 'VirtualAssistants-TheApplication\\nBroadly', 'p<.001‚àó‚àó‚àó)andAIoutputtrust(r=.76', 'Theparticipantcouldinter-\\nvenebyclickingtheInterruptbutton', '0.002 0.173 0.0 0.0 0.0', 'AntinoKim', 'Dulac-ArnoldG', 'RobertLoftin', 'ICCIDS', 'ACMTransactions', 'Martic M', '0.96', 'SocialScience', 'the Google Maps-assisted', 'User Modeling', 'LauraPetrich', 'Computing Systems', 'Yearb Med Inform', 'weuseencoderLLMsfine', 'Workload', 'Aysen Gurcan Namlu', 'Reinforcement Learning', 'Accfinal', 'TomBuchanan', 'IBM SPSS Statistics', 'DeepEpsilon-Greedy', 'StatisticsAnova', 'Working Group', 'Pre-Development', 'Kerstholt JH', 'EPT', 'Niklas', 'NgAY', '0.855 0.631 0.51 0.341', '‚Ä¢ (Thenumberofdesigngoals', 'Netherlands Organisation for Scientific Research', 'AakritiKumar', 'Autonomous Systems', 'AIaccuracy', 'KlasnjaP', 'Current Psychology', 'Wenamethisstrategythreshold-orientedsampling(TOS).Note', 'Acc ‚àà[10%,50%', 'A European Review', 'Clare AS', 'the Creative Commons Attribution Non-Commercial License', 'ADL', 'EXOSOUL Ethical Engine Scheme', 'F:TrafficPsychologyandBehaviour.2020;73:15-28', 'EvitaMarch', 'Ackerman MS', 'Povykalo A', 'Construct QualitativeCodes QuantitativeMeasures(DataRange', 'wewouldexpectthehumantoadheretoA%ofcorrectAIrecommendationsandA%', 'CTSdynamicallyadaptsitsapproach', 'aUniversit√†', 'andContextualThompsonSampling(CTS)[24].Amongthem', 'Anova', 'Sendak', 'ArtificialIntelligence.1999;112(1):181-211', 'Categorical Data', 'USA\\nAbstract', '0.144 0.777 0.335', 'ML', 'ThomasJ', 'Artificial Intelligence, Trust Calibration', 'the 2020 Conference on\\nFairness, Accountability', 'manexpert‚Äôswillingnesstocontributetotheco-creativetoolduringandafterco-creation', 'CheXplain', 'Marsh S', 'Remark 4.1', 'BoneXpert', 'DCODE', 'the 21st International Conference on Intelligent User Interfaces', 'DiscussionandConclusion\\nInthiswork', '‚Ä¢ Wouldyouinvestyourtimeintrainingtouse', 'LiuX', 'Machine\\nLearning in Python', 'Veloso M', 'MMOLAEE', 'LeeS', 'F for s¬Ø', 'VanDuinC', 'Digital', 'MDP', 'ChristinaGeary', 'Community', 'HateSpeech', 'DensityandSaturationcriteria Eventsandtheirpropertiescanbenecessary', 'LLM', 'F', 'therecentlylaunchedChatGPTseemstoprovidehuman', 'ConsideringthecognitiveimpairmentofPwD', 'Larus-Stone N', 'MaxSchemmer', 'Artificial Intelligence - A', 'International\\nJournal of Mental Health', 'SunY', 'AI', 'Robben S', 'communication,17(1):1‚Äì18,2011', 'the 2020 Conference on Fairness, Accountability', 'McGraw-Hill', 'Distributed Dynamic\\nTeam Trust', 'Tejeda', 'Pareek A', 'YunfengZhang', 'GuidelinesforHuman-AIInteractionwhichseekstoguidedinteractionovertimebeyond', 'Acc >Acc', 'CTS 1', 'TheRoleofTrustin\\nHuman-RobotInteraction', 'ActivitiesofDailyLife(ADL)[1,2,3].Suchmonitoringis', 'BaselineResults', 'Social Psychology Bulletin,12(1):75‚Äì80,1986', 'Journal of Research', 'Cece', 'NormanSadeh', 'Kramer RM', '7.00', 'CHI Conference', 'Utrecht University', 'Pak R', 'Ely JW', 'AI AI AI', 'AAAIPress', 'Sepsis Watch', 'MichaelDesmond', 'Exosoul', 'KeenJ', 'Ingeneral', 'the Clinical AI Deployment Cycle', 'SantaClara', 'TREWS', 'GrainedEmotions', 'JMIR Med Inform', 'EHR', 'Defining Trust', 'University of Groningen', 'AdrianoSchimmenti', 'Bellamy RKE', 'J Am Med Inform\\nAssoc', '.082)abc p<.0001\\nConscientiousness', 'Deep Epsilon-Greedy', 'Krishnaswamy N', 'JALT Testing & Evaluation SIG Newsletter', 'CaoS', 'Scheffler', 'Computational Models of Argument', 'InInternational Conference', 'CSCW', 'UnitedKingdom', 'Functioning', 'Keebler JR', 'Argument & Computation', 'object‚Üíspace', 'Value-Aware Active Learning', 'JoshAndres', 'CBR', 'EU', 'Multimodal Semantic Representations', 'Discovery', 'International Data Privacy Law', 'x‚àó=argmin', 'Complianceisoftenusedasabehaviouraldemonstrationoftrustinhuman', 'Grasshopper', 'Personal Differences', 'Atari', 'RahmaniAslM. SketchOpt', 'Armaan A', 'Informatics', 'N', 'SchoolofBusinessand', 'Novak LL', 'Dimensions, Preferences', 'EUR', 'Automated Systems', '0.079 0.034 0.971', 'Phillips EK', 'IEEE', 'n‚Üí‚àû', 'Thispaperdiscusses5stepstowardssuccessfulRLdeployment', 'ProPublica', 'the Netherlands\\nAbstract', 'Ashktorab Z', 'KellyT', 'Ourcontributiontohuman-AIin-', 'Business\\n&InformationSystemsEngineering.2016;58:367-', 'AishaAkbar', 'ElianeSommerfeld', 'Corollary 5', 'PaolaInverardi', 'LiH', 'SME', 'Clusteranalysis.newberrypark,1984', 'Knowledge-Based Systems', 'CERTAINTY 5', 'PatrizioPelliccione', 'AungMH', 'Acosta-ZazuetaG.A3Dshapegenerativemethodforaestheticprod-', 'Hexaco', '0.776 0.487', 'Science of Computer', 'Radiol Med', 'Dark Triad', 'Davidoff S. Trust', 'DST', 'AI-TAM', 'SC', 'Florence, Italy: Association for Computational Linguistics', 'Smith-Renner A', 'BMC Med', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 359\\nReferences', 'Pearson', 'Gutzwiller RS', 'Kiesler S', 'Label 0', 'Correctional Offender Management Profiling', 'Costanza', 'Daronnat S', 'Delroy L Paulhus', 'Cabitza F', 'Amodei D. Deep Reinforcement', 'Riener A', 'ACM', 'Northpointe Inc.', 'TOS', '0.74', 'LucianoFloridi', 'Adaptation and Personalization', 'AI@Trento(FBK-Unitn', 'Initial Ethical\\nInference Engine Profile', 'the Interdependence of Reliance\\nBehavior', 'Horizon 2020', 'Hatice Ferhan Odabasi', 'NASA', 'ancinghowimpactfulasuccessfulsolutionwillbe', 'BMC Med Inform Decis\\nMak', 'Table 4.3', 'Forthisreason', 'Feedback Interface', 'the Small COMPAS Data', 'EXOSOULisbasedonthenotion', 'Business Ethics', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches', 'RLOps', 'AcademicPress;2021.p.143', 'BMC Med Inform Decis Mak', 'Trust Through Clinical Trials and Peer Reviewed Publications\\nPrior', 'Department of Information', 'x‚àó=argmin x‚ààD', 'DigitalBehaviors', 'a Philips Experience Design', 'F :s.', 'McDaniel', 'Koenigsberg MR', 'AI\\nAcc ‚àà', 'Agrawal M', 'NY', 'Cole', 'Trust Through Clinical Involvement', 'Thelocationmapislaidonagrid', 'Wang', 'cancelevel:‚àó‚àó‚àóp<.001,‚àó‚àóp<.01,‚àóp<.05', 'Post-Deployment\\n3.5.1', 'Success', 'De Keizer NF', 'Siewiorek DP', 'AnupamDas', 'Racedimension', 'LiP', 'andneedsofPwD', 'doi.org/10.1109/IROS.2017.8202133', 'Delft University of Technology', 'Trust Through Public Accountability', '‚Ä¢ Itisverycapableofperformingitsjob', 'Trust Through the AI Development Team', 'USA: Association', 'Linear Upper Confidence Bound', 'TAM', 'MoharebM', 'M.E.Taylor/ReinforcementLearningRequiresHuman-in-the-LoopFramingandApproaches 357', 'the EFMI Working Group for Assessment of Health Information Systems', '‚é™‚é™‚é®(A correct+O correct)‚àí(100%‚àíAcc AI‚àíA', 'Table 2', 'Massimiliano', 'CHI\\nConference on Human Factors', 'the Dark Triad Dirty', 'European Dementia Monitor', 'HelenaVasconcelos', 'LiL', 'the COMPAS Recidivism Algorithm', 'Narcissism', 'BinLiu', 'U.S. Food and Drug Administration', 'Computing\\nSystems', 'A ‚â• Acc', 'Thiscodecould ‚Ä¢ Idonotfeeloutofcontrolinworkingwithit', 'Strohm', 'l', '.315 F', 'CTS', 'MDPandtrytolearnahigherperformingpolicy', 'CRC Press', 'USA,2020.AssociationforComputingMachinery', 'SPIE\\nProceedings', 'Softethicsandthegovernanceofthedigital', 'AarhusUniversity\\nAbstract', 'Building Trust', 'Fussell SR', 'Dementia', 'AI\\nGeometrically', 'QR-Code', '0.045 0.02', 'Fig.1', 'Wilcox L', 'BTOS', 'Machine Learning', 'The Journal of\\nEducation', '323', 'Clinical AI', 'Nishikawa', 'Dazeley R', 'the Unit of Analysis of Trust', 'InthiscontextthemultidisciplinaryprojectExosoul', 'GroveAJ', 'IOS Press', 'PwD', 'Conf\\nProc Ethnogr Prax Ind Conf', 'Bart', 'F, Tischler V', 'Acc ‚àíA >0.Theminimumachievabledecision', 'Yearb\\nMed Inform', 'means‚Äô[10,15', 'US Airlines', 'Inference Engine\\nFill', 'International Journal of Management and Bussiness', 'LeeJD.ModelingGoalAlignmentinHuman-AITeaming', 'UserBot', 'COMPAS', 'CHI Conference on Human Factors', 'AutonomousSystems', 'Fitzpatrick', 'Acc <Acc', 'Althoefer\\nK', '‚Ä¢ Q2', 'GP', 'Hum Factors', 'Ferna¬¥ndez-LealA.Human-', 'NewYork', '6.04 7.00', 'CB', 'un', 'Intelligent Systems Technical Conference', 'Journal of Machine Learning Research', 'MochenYang', 'AndreaPASSERINIaandFabioCASATIc', 'Thompson WR', 'American Institute of Aeronautics', 'hasPlace', 'Trust Through the Training of the AI', 'Cummings ML', 'Social Computing', 'Healthcare', 'PadhraicSmyth', 'al', 'NWO', 'CarinaPaine', 'Computer', 'Paul T Costa', 'F ‚â∫ M', 'CA', 'Keywords', 'Northepointe, Inc.', 'Chernova S', 'CeliaMoore', '‚Ä¢ Certainty', 'Frontiers of Theory and Research']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from all PDFs in a folder\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    extracted_texts = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder, filename)\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \" \".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "                extracted_texts.append(text)\n",
    "    return extracted_texts\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Extract key terms using TF-IDF\n",
    "def extract_keywords_tfidf(texts, num_keywords=20):\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words=\"english\", ngram_range=(1,2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).tolist()[0]\n",
    "    keyword_scores = dict(zip(feature_names, tfidf_scores))\n",
    "    sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [keyword for keyword, _ in sorted_keywords[:num_keywords]]\n",
    "\n",
    "# Extract named entities (AI-related terms, datasets, models)\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {\"ORG\", \"PRODUCT\", \"EVENT\"}]\n",
    "    return list(set(entities))\n",
    "\n",
    "# Provide the path to your PDFs folder\n",
    "pdf_folder_path = \"pdfs\"  # Update this to your actual folder path\n",
    "\n",
    "# Extract and process text\n",
    "raw_texts = extract_text_from_pdfs(pdf_folder_path)\n",
    "processed_texts = [preprocess_text(text) for text in raw_texts]\n",
    "\n",
    "# Get key terms\n",
    "tfidf_keywords = extract_keywords_tfidf(processed_texts)\n",
    "named_entities = []\n",
    "for text in raw_texts:\n",
    "    named_entities.extend(extract_named_entities(text))\n",
    "\n",
    "# Print extracted key concepts\n",
    "print(\"üîπ Extracted Key Concepts:\")\n",
    "print(\"üìå Top Keywords:\", tfidf_keywords)\n",
    "print(\"üìå Named Entities:\", list(set(named_entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing Classes: {'wtchangetalhumancenteredaifordementiacare', 'model', 'figure', 'learning', 'available', 'human', 'cid', 'user', 'cluster', 'data', 'al', 'uncertainty', 'pvossenetalaconversationalagent', 'trust', 'sampling', 'rl', 'design', 'acc', 'agent', 'ai'}\n",
      "‚ùå Missing Object Properties: {'wtchangetalhumancenteredaifordementiacare', 'model', 'figure', 'learning', 'available', 'human', 'cid', 'user', 'cluster', 'data', 'al', 'uncertainty', 'pvossenetalaconversationalagent', 'trust', 'sampling', 'rl', 'design', 'acc', 'agent', 'ai'}\n",
      "‚ùå Missing Data Properties: {'wtchangetalhumancenteredaifordementiacare', 'model', 'figure', 'learning', 'available', 'human', 'cid', 'user', 'cluster', 'data', 'al', 'uncertainty', 'pvossenetalaconversationalagent', 'trust', 'sampling', 'rl', 'design', 'acc', 'agent', 'ai'}\n"
     ]
    }
   ],
   "source": [
    "# Convert extracted keywords to a set for comparison\n",
    "extracted_keywords_set = set(tfidf_keywords)\n",
    "extracted_named_entities_set = set(named_entities)\n",
    "\n",
    "# Convert existing ontology classes and properties into sets\n",
    "existing_classes_set = set([s.split(\"#\")[-1] for s in existing_classes])\n",
    "existing_object_properties_set = set([s.split(\"#\")[-1] for s in existing_object_properties])\n",
    "existing_data_properties_set = set([s.split(\"#\")[-1] for s in existing_data_properties])\n",
    "\n",
    "# Find missing classes\n",
    "missing_classes = extracted_keywords_set - existing_classes_set\n",
    "\n",
    "# Find missing object properties\n",
    "missing_object_properties = extracted_keywords_set - existing_object_properties_set\n",
    "\n",
    "# Find missing data properties (assuming no existing ones)\n",
    "missing_data_properties = extracted_keywords_set - existing_data_properties_set if existing_data_properties else extracted_keywords_set\n",
    "\n",
    "# Print results\n",
    "print(\"‚ùå Missing Classes:\", missing_classes)\n",
    "print(\"‚ùå Missing Object Properties:\", missing_object_properties)\n",
    "print(\"‚ùå Missing Data Properties:\", missing_data_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated successfully! Saved as updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "from rdflib import URIRef, Graph, Namespace, RDF, RDFS, OWL, XSD, Literal\n",
    "\n",
    "# Define the namespace (update it based on your ontology's namespace)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Function to clean concept names (replace spaces and special chars, ensure valid URI)\n",
    "def clean_name(name):\n",
    "    return urllib.parse.quote(name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\"))\n",
    "\n",
    "# Define safe mappings for problematic words\n",
    "safe_mappings = {\n",
    "    \"acc\": \"Accuracy\",\n",
    "    \"ai\": \"Artificial_Intelligence\",\n",
    "    \"rl\": \"Reinforcement_Learning\",\n",
    "    \"cid\": \"Concept_ID\",\n",
    "    \"al\": \"Algorithm\",\n",
    "    \"pvossenetalaconversationalagent\": \"Conversational_Agent\",\n",
    "    \"wtchangetalhumancenteredaifordementiacare\": \"Human_Centered_AI_Dementia\",\n",
    "}\n",
    "\n",
    "# Function to get safe name\n",
    "def get_safe_name(name):\n",
    "    return safe_mappings.get(name, clean_name(name))\n",
    "\n",
    "# Add missing classes\n",
    "for class_name in missing_classes:\n",
    "    class_name_clean = get_safe_name(class_name)\n",
    "    class_uri = HI[class_name_clean]\n",
    "    g.add((class_uri, RDF.type, OWL.Class))\n",
    "    g.add((class_uri, RDFS.label, Literal(class_name_clean)))  \n",
    "    g.add((class_uri, RDFS.comment, Literal(f\"Automatically added class: {class_name_clean}\")))  \n",
    "\n",
    "# Add missing object properties\n",
    "for prop in missing_object_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.ObjectProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, HI[\"AIConcept\"]))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added object property: {prop_clean}\")))\n",
    "\n",
    "# Add missing data properties\n",
    "for prop in missing_data_properties:\n",
    "    prop_clean = get_safe_name(prop)\n",
    "    prop_uri = HI[prop_clean]\n",
    "    g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((prop_uri, RDFS.domain, HI[\"AIModel\"]))  \n",
    "    g.add((prop_uri, RDFS.range, XSD.string))  \n",
    "    g.add((prop_uri, RDFS.label, Literal(prop_clean)))  \n",
    "    g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added data property: {prop_clean}\")))  \n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated successfully! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evoman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
