{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ **Existing Classes:** 15\n",
      "['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/EthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InformationProcessing', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Interaction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Scenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Capability']\n",
      "\n",
      "‚úÖ **Existing Subclasses:** 2\n",
      "[('http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor'), ('http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor')]\n",
      "\n",
      "‚úÖ **Existing Object Properties:** 13\n",
      "['http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/capability', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/context', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/domain', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/endgoal', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasEthicalConsideration', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasInteraction', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/inScenario', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/informationMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactingAgent', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionMethod', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionTask', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingInformation', 'http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingTask']\n",
      "\n",
      "‚úÖ **Existing Data Properties:** 0\n",
      "[]\n",
      "\n",
      "‚úÖ **Existing Research Papers:** 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, OWL, RDFS\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"  # Update with the correct path\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "\n",
    "# Extract all Classes\n",
    "existing_classes = [str(cls) for cls in g.subjects(RDF.type, OWL.Class)]\n",
    "\n",
    "# Extract all Subclasses (Fix: Ensure child-parent relationships are captured)\n",
    "existing_subclasses = []\n",
    "for child in g.subjects(RDFS.subClassOf, None):\n",
    "    for parent in g.objects(subject=child, predicate=RDFS.subClassOf):\n",
    "        existing_subclasses.append((str(child), str(parent)))\n",
    "\n",
    "# Extract all Object Properties\n",
    "existing_object_properties = [str(prop) for prop in g.subjects(RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "# Extract all Data Properties\n",
    "existing_data_properties = [str(prop) for prop in g.subjects(RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "# Extract all Research Papers (Instances of ResearchPaper class)\n",
    "existing_papers = [str(paper) for paper in g.subjects(RDF.type, HI[\"ResearchPaper\"])]\n",
    "\n",
    "# Print Summary\n",
    "print(\"\\n‚úÖ **Existing Classes:**\", len(existing_classes))\n",
    "print(existing_classes)\n",
    "\n",
    "print(\"\\n‚úÖ **Existing Subclasses:**\", len(existing_subclasses))\n",
    "print(existing_subclasses)\n",
    "\n",
    "print(\"\\n‚úÖ **Existing Object Properties:**\", len(existing_object_properties))\n",
    "print(existing_object_properties)\n",
    "\n",
    "print(\"\\n‚úÖ **Existing Data Properties:**\", len(existing_data_properties))\n",
    "print(existing_data_properties)\n",
    "\n",
    "print(\"\\n‚úÖ **Existing Research Papers:**\", len(existing_papers))\n",
    "print(existing_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated successfully! Saved as updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, OWL, RDFS, XSD, Literal\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"  # Update with the correct path\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# üõ† Define Required Data Properties\n",
    "data_properties = {\n",
    "    \"hasTitle\": XSD.string,\n",
    "    \"hasAuthor\": XSD.string,\n",
    "    \"hasYear\": XSD.integer,\n",
    "    \"hasDOI\": XSD.string,\n",
    "    \"hasAbstract\": XSD.string,\n",
    "    \"hasVenue\": XSD.string,\n",
    "    \"hasKeywords\": XSD.string,\n",
    "    \"hasExternalLink\": XSD.string\n",
    "}\n",
    "\n",
    "# üõ† Add Data Properties (if they don‚Äôt exist)\n",
    "for prop, dtype in data_properties.items():\n",
    "    prop_uri = HI[prop]\n",
    "    if (prop_uri, RDF.type, OWL.DatatypeProperty) not in g:\n",
    "        g.add((prop_uri, RDF.type, OWL.DatatypeProperty))\n",
    "        g.add((prop_uri, RDFS.domain, HI[\"ResearchPaper\"]))  # Set domain to ResearchPaper\n",
    "        g.add((prop_uri, RDFS.range, dtype))\n",
    "        g.add((prop_uri, RDFS.label, Literal(prop)))\n",
    "        g.add((prop_uri, RDFS.comment, Literal(f\"Automatically added data property: {prop}\")))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated successfully! Saved as {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracted Research Papers Metadata:\n",
      "üìå On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making (2023)\n",
      "   üìù Authors: Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, Gerhard Satzger\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230074\n",
      "   üåê URL: https://doi.org/10.3233/faia230074\n",
      "   üìë Keywords: ['AI']\n",
      "\n",
      "üìå Value-Aware Active Learning (2023)\n",
      "   üìù Authors: Burcu Sayin, Jie Yang, Andrea Passerini, Fabio Casati\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230085\n",
      "   üåê URL: https://doi.org/10.3233/faia230085\n",
      "   üìë Keywords: []\n",
      "\n",
      "üìå Human-Centered AI for Dementia Care: Using Reinforcement Learning for Personalized Interventions Support in Eating and Drinking Scenarios (2024)\n",
      "   üìù Authors: Wen-Tseng Chang, Shihan Wang, Stephanie Kramer, Michel Oey, Somaya Ben Allouch\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240185\n",
      "   üåê URL: https://doi.org/10.3233/faia240185\n",
      "   üìë Keywords: ['PwD', 'RL']\n",
      "\n",
      "üìå Reinforcement Learning Requires Human-in-the-Loop Framing and Approaches (2023)\n",
      "   üìù Authors: Matthew E. Taylor\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230098\n",
      "   üåê URL: https://doi.org/10.3233/faia230098\n",
      "   üìë Keywords: []\n",
      "\n",
      "üìå Trust in Clinical AI: Expanding the Unit of Analysis (2022)\n",
      "   üìù Authors: Jacob T. Browne, Saskia Bakker, Bin Yu, Peter Lloyd, Somaya Ben Allouch\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220192\n",
      "   üåê URL: https://doi.org/10.3233/faia220192\n",
      "   üìë Keywords: []\n",
      "\n",
      "üìå A Hybrid Intelligence Approach to Training Generative Design Assistants: Partnership Between Human Experts and AI Enhanced Co-Creative Tools (2023)\n",
      "   üìù Authors: Yaoli Mao, Janet Rafner, Yi Wang, Jacob Sherson\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia230078\n",
      "   üåê URL: https://doi.org/10.3233/faia230078\n",
      "   üìë Keywords: ['Hybrid Intelligence Technology Acceptance Model', 'GD', 'AI']\n",
      "\n",
      "üìå Landmarks in Case-Based Reasoning: From Theory to Data (2022)\n",
      "   üìù Authors: Wijnand van Woerkom, Davide Grossi, Henry Prakken, Bart Verheij\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220200\n",
      "   üåê URL: https://doi.org/10.3233/faia220200\n",
      "   üìë Keywords: ['AI &amp']\n",
      "\n",
      "üìå A Conversational Agent for Structured Diary Construction Enabling Monitoring of Functioning &amp; Well-Being (2024)\n",
      "   üìù Authors: Piek Vossen, Selene B√°ez Santamar√≠a, Thomas Baier\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240204\n",
      "   üåê URL: https://doi.org/10.3233/faia240204\n",
      "   üìë Keywords: ['MIT', 'GitHub']\n",
      "\n",
      "üìå Unknown (None)\n",
      "   üìù Authors: \n",
      "   üìñ Venue: Unknown\n",
      "   üîó DOI: \n",
      "   üåê URL: \n",
      "   üìë Keywords: []\n",
      "\n",
      "üìå Exemplars and Counterexemplars Explanations for Skin Lesion Classifiers (2022)\n",
      "   üìù Authors: Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia220209\n",
      "   üåê URL: https://doi.org/10.3233/faia220209\n",
      "   üìë Keywords: []\n",
      "\n",
      "üìå Unreflected Acceptance ‚Äì Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education (2024)\n",
      "   üìù Authors: Lars Krupp, Steffen Steinert, Maximilian Kiefer-Emmanouilidis, Karina E. Avila, Paul Lukowicz, Jochen Kuhn, Stefan K√ºchemann, Jakob Karolus\n",
      "   üìñ Venue: Frontiers in Artificial Intelligence and Applications\n",
      "   üîó DOI: 10.3233/faia240195\n",
      "   üåê URL: https://doi.org/10.3233/faia240195\n",
      "   üìë Keywords: []\n",
      "\n",
      "‚úÖ Extracted metadata saved in extracted_metadata.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pdfplumber\n",
    "import urllib.parse\n",
    "import spacy\n",
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, XSD, Literal\n",
    "\n",
    "# üìå Load NLP Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# üìå Load Ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# üìå Define Namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# üìå Ensure ResearchPaper Class Exists\n",
    "ResearchPaper = HI[\"ResearchPaper\"]\n",
    "if (ResearchPaper, RDF.type, OWL.Class) not in g:\n",
    "    g.add((ResearchPaper, RDF.type, OWL.Class))\n",
    "    g.add((ResearchPaper, RDFS.label, Literal(\"Research Paper\")))\n",
    "    g.add((ResearchPaper, RDFS.comment, Literal(\"Class representing research papers in the ontology.\")))\n",
    "\n",
    "# üìå Extract Metadata from CrossRef API using DOIs\n",
    "def get_metadata_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url).json()\n",
    "    if \"message\" in response:\n",
    "        metadata = response[\"message\"]\n",
    "        return {\n",
    "            \"title\": metadata.get(\"title\", [\"Unknown\"])[0],\n",
    "            \"authors\": \", \".join([author.get(\"given\", \"\") + \" \" + author.get(\"family\", \"\") \n",
    "                                  for author in metadata.get(\"author\", [])]),\n",
    "            \"year\": metadata.get(\"published-print\", {}).get(\"date-parts\", [[None]])[0][0],\n",
    "            \"venue\": metadata.get(\"container-title\", [\"Unknown\"])[0],\n",
    "            \"doi\": metadata.get(\"DOI\", \"\"),\n",
    "            \"url\": metadata.get(\"URL\", \"\"),\n",
    "            \"abstract\": metadata.get(\"abstract\", \"No abstract available\")\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# üìå Fetch Metadata using Semantic Scholar API (Backup Method)\n",
    "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "def get_paper_metadata(title):\n",
    "    params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"title,authors,year,doi,venue,abstract\"\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "            paper = data[\"data\"][0]  \n",
    "            return {\n",
    "                \"title\": paper.get(\"title\", \"N/A\"),\n",
    "                \"authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])]),\n",
    "                \"year\": paper.get(\"year\", \"N/A\"),\n",
    "                \"doi\": paper.get(\"doi\", \"N/A\"),\n",
    "                \"venue\": paper.get(\"venue\", \"N/A\"),\n",
    "                \"abstract\": paper.get(\"abstract\", \"N/A\")\n",
    "            }\n",
    "    else:\n",
    "        print(f\"‚ùå Error fetching metadata for: {title}\")\n",
    "    return None\n",
    "\n",
    "# üìå Extract Keywords & Topics Using TF-IDF & NLP\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    return list(set([ent.text for ent in doc.ents if ent.label_ in {\"ORG\", \"PRODUCT\", \"EVENT\"}]))\n",
    "\n",
    "# üìå Define DOI List\n",
    "doi_list = [\n",
    "    \"10.3233/FAIA230074\", \"10.3233/FAIA230085\", \"10.3233/FAIA240185\",\n",
    "    \"10.3233/FAIA230098\", \"10.3233/FAIA220192\", \"10.3233/FAIA230078\",\n",
    "    \"10.3233/FAIA220200\", \"10.3233/FAIA240204\", \"\", \"10.3233/FAIA220209\",\n",
    "    \"10.3233/FAIA240195\"\n",
    "]\n",
    "\n",
    "# üìå Extract Metadata\n",
    "papers_metadata = [get_metadata_from_doi(doi) for doi in doi_list]\n",
    "\n",
    "# üìå Extract Keywords Using NLP\n",
    "for paper in papers_metadata:\n",
    "    if paper and \"abstract\" in paper:\n",
    "        paper[\"keywords\"] = extract_keywords(paper[\"abstract\"])\n",
    "    else:\n",
    "        paper[\"keywords\"] = \"No keywords available\"\n",
    "\n",
    "# üìå Print Extracted Metadata\n",
    "print(\"üìÑ Extracted Research Papers Metadata:\")\n",
    "for paper in papers_metadata:\n",
    "    print(f\"üìå {paper['title']} ({paper['year']})\")\n",
    "    print(f\"   üìù Authors: {paper['authors']}\")\n",
    "    print(f\"   üìñ Venue: {paper['venue']}\")\n",
    "    print(f\"   üîó DOI: {paper['doi']}\")\n",
    "    print(f\"   üåê URL: {paper['url']}\")\n",
    "    print(f\"   üìë Keywords: {paper['keywords']}\\n\")\n",
    "\n",
    "# üìå Save Extracted Metadata for Next Step\n",
    "metadata_path = \"extracted_metadata.json\"\n",
    "import json\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(papers_metadata, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Extracted metadata saved in {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated with subclass hierarchy! Saved as updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, XSD, Literal\n",
    "\n",
    "# üìå Load Ontology Path\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "\n",
    "# üìå Load Ontology Graph\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# üìå Define Namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# üìå Define Parent Classes (Ensure they exist)\n",
    "Actor = HI[\"Actor\"]\n",
    "AIConcept = HI[\"AIConcept\"]\n",
    "ResearchPaper = HI[\"ResearchPaper\"]\n",
    "\n",
    "# üìå Add Parent Classes if missing\n",
    "for cls in [Actor, AIConcept, ResearchPaper]:\n",
    "    if (cls, RDF.type, OWL.Class) not in g:\n",
    "        g.add((cls, RDF.type, OWL.Class))\n",
    "\n",
    "# üìå Define New Subclasses\n",
    "subclasses = {\n",
    "    \"Human\": \"Actor\",\n",
    "    \"ArtificialAgent\": \"Actor\",\n",
    "    \"Researcher\": \"Actor\",\n",
    "    \"MachineLearning\": \"AIConcept\",\n",
    "    \"SupervisedLearning\": \"MachineLearning\",\n",
    "    \"UnsupervisedLearning\": \"MachineLearning\",\n",
    "    \"ReinforcementLearning\": \"MachineLearning\",\n",
    "    \"JournalPaper\": \"ResearchPaper\",\n",
    "    \"ConferencePaper\": \"ResearchPaper\",\n",
    "}\n",
    "\n",
    "# üìå Add Subclasses to Ontology\n",
    "for subclass, parent in subclasses.items():\n",
    "    subclass_uri = HI[subclass]\n",
    "    parent_uri = HI[parent]\n",
    "    \n",
    "    if (subclass_uri, RDF.type, OWL.Class) not in g:\n",
    "        g.add((subclass_uri, RDF.type, OWL.Class))\n",
    "        g.add((subclass_uri, RDFS.label, Literal(subclass)))\n",
    "    \n",
    "    g.add((subclass_uri, RDFS.subClassOf, parent_uri))\n",
    "\n",
    "# üìå Save Updated Ontology\n",
    "g.serialize(destination=ontology_path, format=\"turtle\")\n",
    "print(f\"‚úÖ Ontology updated with subclass hierarchy! Saved as {ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ **Extracted Classes:**\n",
      "  - http://www.semanticweb.org/hi_ontology#ArtificialAgent\n",
      "  - http://www.semanticweb.org/hi_ontology#ConferencePaper\n",
      "  - http://www.semanticweb.org/hi_ontology#Human\n",
      "  - http://www.semanticweb.org/hi_ontology#JournalPaper\n",
      "  - http://www.semanticweb.org/hi_ontology#ReinforcementLearning\n",
      "  - http://www.semanticweb.org/hi_ontology#Researcher\n",
      "  - http://www.semanticweb.org/hi_ontology#SupervisedLearning\n",
      "  - http://www.semanticweb.org/hi_ontology#UnsupervisedLearning\n",
      "  - http://www.semanticweb.org/hi_ontology#AIConcept\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Context\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Domain\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Endgoal\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/EthicalConsideration\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent\n",
      "  - http://www.semanticweb.org/hi_ontology#Actor\n",
      "  - http://www.semanticweb.org/hi_ontology#MachineLearning\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InformationProcessing\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Interaction\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionTask\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingMethod\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ProcessingTask\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/InteractionMethod\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Scenario\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Capability\n",
      "  - http://www.semanticweb.org/hi_ontology#ResearchPaper\n",
      "\n",
      "‚úÖ **Extracted Subclasses:**\n",
      "  - http://www.semanticweb.org/hi_ontology#ArtificialAgent ‚äÜ http://www.semanticweb.org/hi_ontology#Actor\n",
      "  - http://www.semanticweb.org/hi_ontology#Human ‚äÜ http://www.semanticweb.org/hi_ontology#Actor\n",
      "  - http://www.semanticweb.org/hi_ontology#Researcher ‚äÜ http://www.semanticweb.org/hi_ontology#Actor\n",
      "  - http://www.semanticweb.org/hi_ontology#ConferencePaper ‚äÜ http://www.semanticweb.org/hi_ontology#ResearchPaper\n",
      "  - http://www.semanticweb.org/hi_ontology#JournalPaper ‚äÜ http://www.semanticweb.org/hi_ontology#ResearchPaper\n",
      "  - http://www.semanticweb.org/hi_ontology#ReinforcementLearning ‚äÜ http://www.semanticweb.org/hi_ontology#MachineLearning\n",
      "  - http://www.semanticweb.org/hi_ontology#SupervisedLearning ‚äÜ http://www.semanticweb.org/hi_ontology#MachineLearning\n",
      "  - http://www.semanticweb.org/hi_ontology#UnsupervisedLearning ‚äÜ http://www.semanticweb.org/hi_ontology#MachineLearning\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/ArtificialAgent ‚äÜ http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Human ‚äÜ http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/Actor\n",
      "  - http://www.semanticweb.org/hi_ontology#MachineLearning ‚äÜ http://www.semanticweb.org/hi_ontology#AIConcept\n",
      "\n",
      "‚úÖ **Extracted Object Properties:**\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/capability\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/context\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/domain\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/endgoal\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasEthicalConsideration\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/hasInteraction\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/inScenario\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/informationMethod\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactingAgent\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionMethod\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/interactionTask\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingInformation\n",
      "  - http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/processingTask\n",
      "\n",
      "‚úÖ **Extracted Data Properties:**\n",
      "  - http://www.semanticweb.org/hi_ontology#hasAbstract\n",
      "  - http://www.semanticweb.org/hi_ontology#hasAuthor\n",
      "  - http://www.semanticweb.org/hi_ontology#hasDOI\n",
      "  - http://www.semanticweb.org/hi_ontology#hasExternalLink\n",
      "  - http://www.semanticweb.org/hi_ontology#hasKeywords\n",
      "  - http://www.semanticweb.org/hi_ontology#hasTitle\n",
      "  - http://www.semanticweb.org/hi_ontology#hasVenue\n",
      "  - http://www.semanticweb.org/hi_ontology#hasYear\n",
      "\n",
      "‚úÖ **Extracted Research Papers:**\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL\n",
    "\n",
    "# üìå Load the updated ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# üìå Define Namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/hi_ontology#\")\n",
    "\n",
    "# üìå Extract Classes & Subclasses\n",
    "existing_classes = [str(cls) for cls in g.subjects(RDF.type, OWL.Class)]\n",
    "existing_subclasses = [(str(child), str(parent)) for child, _, parent in g.triples((None, RDFS.subClassOf, None))]\n",
    "\n",
    "# üìå Print Class & Subclass Hierarchy\n",
    "print(\"\\n‚úÖ **Extracted Classes:**\")\n",
    "for cls in existing_classes:\n",
    "    print(f\"  - {cls}\")\n",
    "\n",
    "print(\"\\n‚úÖ **Extracted Subclasses:**\")\n",
    "for child, parent in existing_subclasses:\n",
    "    print(f\"  - {child} ‚äÜ {parent}\")\n",
    "\n",
    "# üìå Extract Object & Data Properties\n",
    "existing_object_properties = [str(prop) for prop in g.subjects(RDF.type, OWL.ObjectProperty)]\n",
    "existing_data_properties = [str(prop) for prop in g.subjects(RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "# üìå Print Object & Data Properties\n",
    "print(\"\\n‚úÖ **Extracted Object Properties:**\")\n",
    "for prop in existing_object_properties:\n",
    "    print(f\"  - {prop}\")\n",
    "\n",
    "print(\"\\n‚úÖ **Extracted Data Properties:**\")\n",
    "for prop in existing_data_properties:\n",
    "    print(f\"  - {prop}\")\n",
    "\n",
    "# üìå Extract Research Papers\n",
    "research_papers = [str(paper) for paper in g.subjects(RDF.type, HI[\"ResearchPaper\"])]\n",
    "\n",
    "print(\"\\n‚úÖ **Extracted Research Papers:**\")\n",
    "for paper in research_papers:\n",
    "    print(f\"  - {paper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, Literal, XSD, BNode\n",
    "\n",
    "# Load the ontology\n",
    "ontology_path = \"hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define the namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "### Step 1: Add New Subclasses ###\n",
    "g.add((HI.MachineLearningSystem, RDFS.subClassOf, HI.ArtificialAgent))\n",
    "g.add((HI.HumanExpert, RDFS.subClassOf, HI.Human))\n",
    "g.add((HI.InteractiveRobot, RDFS.subClassOf, HI.Robot))\n",
    "g.add((HI.CognitiveSystem, RDFS.subClassOf, HI.Intelligent_System))\n",
    "\n",
    "### Step 2: Add Complex Object Properties with Restrictions ###\n",
    "g.add((HI.hasProcessingTask, RDF.type, OWL.ObjectProperty))\n",
    "g.add((HI.hasProcessingTask, RDFS.domain, HI.ArtificialAgent))\n",
    "g.add((HI.hasProcessingTask, RDFS.range, HI.ProcessingTask))\n",
    "\n",
    "# ‚úÖ Define OWL Restriction on ArtificialAgent (minCardinality 1)\n",
    "restriction_bnode = BNode()  # Create a blank node for restriction\n",
    "g.add((restriction_bnode, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_bnode, OWL.onProperty, HI.hasProcessingTask))\n",
    "g.add((restriction_bnode, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "g.add((HI.ArtificialAgent, RDFS.subClassOf, restriction_bnode))  # Attach restriction to class\n",
    "\n",
    "### Step 3: Add Datatype Properties ###\n",
    "g.add((HI.processingSpeed, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((HI.processingSpeed, RDFS.domain, HI.ArtificialAgent))\n",
    "g.add((HI.processingSpeed, RDFS.range, XSD.float))\n",
    "\n",
    "g.add((HI.accuracyRate, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((HI.accuracyRate, RDFS.domain, HI.Intelligent_System))\n",
    "g.add((HI.accuracyRate, RDFS.range, XSD.decimal))\n",
    "\n",
    "### Step 4: Add New Individuals with Properties ###\n",
    "g.add((HI.DeepLearningAI, RDF.type, HI.MachineLearningSystem))\n",
    "g.add((HI.DeepLearningAI, HI.processingSpeed, Literal(0.85, datatype=XSD.float)))\n",
    "g.add((HI.DeepLearningAI, HI.accuracyRate, Literal(98.5, datatype=XSD.decimal)))\n",
    "\n",
    "g.add((HI.ExpertUser, RDF.type, HI.HumanExpert))\n",
    "g.add((HI.ExpertUser, HI.capability, HI.Adaptiveness))\n",
    "\n",
    "g.add((HI.AssistantRobot, RDF.type, HI.InteractiveRobot))\n",
    "g.add((HI.AssistantRobot, HI.capability, HI.Collaborativeness))\n",
    "\n",
    "### Step 5: Save Updated Ontology ###\n",
    "updated_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, RDFS, OWL, Literal, XSD\n",
    "\n",
    "# Load existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define namespace\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "### Step 1: Define New Classes (Subclasses) ###\n",
    "g.add((HI.Human_Agent, RDF.type, OWL.Class))\n",
    "g.add((HI.Artificial_Intelligence, RDF.type, OWL.Class))\n",
    "g.add((HI.Ethical_Challenge, RDF.type, OWL.Class))\n",
    "\n",
    "# Define subclass relationships\n",
    "g.add((HI.Human_Agent, RDFS.subClassOf, HI.Actor))\n",
    "g.add((HI.Artificial_Intelligence, RDFS.subClassOf, HI.ArtificialAgent))\n",
    "g.add((HI.Autonomous_AI, RDFS.subClassOf, HI.Artificial_Intelligence))\n",
    "g.add((HI.Ethical_AI, RDFS.subClassOf, HI.Artificial_Intelligence))\n",
    "g.add((HI.Explainable_AI, RDFS.subClassOf, HI.Artificial_Intelligence))\n",
    "\n",
    "### Step 2: Define Object Properties ###\n",
    "g.add((HI.interactsWith, RDF.type, OWL.ObjectProperty))\n",
    "g.add((HI.interactsWith, RDFS.domain, HI.Human_Agent))\n",
    "g.add((HI.interactsWith, RDFS.range, HI.Artificial_Intelligence))\n",
    "\n",
    "g.add((HI.hasEthicalChallenge, RDF.type, OWL.ObjectProperty))\n",
    "g.add((HI.hasEthicalChallenge, RDFS.domain, HI.Artificial_Intelligence))\n",
    "g.add((HI.hasEthicalChallenge, RDFS.range, HI.Ethical_Challenge))\n",
    "\n",
    "### Step 3: Define Datatype Properties ###\n",
    "g.add((HI.AI_Reliability, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((HI.AI_Reliability, RDFS.domain, HI.Artificial_Intelligence))\n",
    "g.add((HI.AI_Reliability, RDFS.range, XSD.float))\n",
    "\n",
    "g.add((HI.Ethical_Score, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((HI.Ethical_Score, RDFS.domain, HI.Artificial_Intelligence))\n",
    "g.add((HI.Ethical_Score, RDFS.range, XSD.integer))\n",
    "\n",
    "### Step 4: Define Restrictions (e.g., Min Cardinality) ###\n",
    "restriction_bnode = HI.Restriction1\n",
    "g.add((restriction_bnode, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_bnode, OWL.onProperty, HI.interactsWith))\n",
    "g.add((restriction_bnode, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "g.add((HI.Human_Agent, RDFS.subClassOf, restriction_bnode))\n",
    "\n",
    "### Step 5: Add Instances ###\n",
    "g.add((HI.Chatbot_X, RDF.type, HI.Explainable_AI))\n",
    "g.add((HI.Chatbot_X, HI.AI_Reliability, Literal(0.85, datatype=XSD.float)))\n",
    "g.add((HI.Chatbot_X, HI.Ethical_Score, Literal(7, datatype=XSD.integer)))\n",
    "\n",
    "g.add((HI.Doctor_Y, RDF.type, HI.Human_Agent))\n",
    "g.add((HI.Doctor_Y, HI.interactsWith, HI.Chatbot_X))\n",
    "\n",
    "g.add((HI.Bias_in_AI, RDF.type, HI.Ethical_Challenge))\n",
    "g.add((HI.Chatbot_X, HI.hasEthicalChallenge, HI.Bias_in_AI))\n",
    "\n",
    "### Save updated ontology ###\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (consistent with previous updates)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes\n",
    "new_classes = {\n",
    "    \"AI_Assisted_DecisionMaking\": HI.AIConcept,\n",
    "    \"Reliance_Behavior\": HI.AI_Assisted_DecisionMaking,\n",
    "    \"Human_AI_Interaction\": HI.AIConcept,\n",
    "    \"TrustCalibration\": HI.Reliance_Behavior,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties\n",
    "new_object_properties = {\n",
    "    \"hasRelianceBehavior\": (HI.Human_AI_Interaction, HI.Reliance_Behavior),\n",
    "    \"affectsDecisionMaking\": (HI.Reliance_Behavior, HI.AI_Assisted_DecisionMaking),\n",
    "    \"requiresTrustCalibration\": (HI.AI_Assisted_DecisionMaking, HI.TrustCalibration),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties\n",
    "new_datatype_properties = {\n",
    "    \"trustScore\": (HI.Human_AI_Interaction, XSD.float),\n",
    "    \"adherenceLevel\": (HI.Reliance_Behavior, XSD.integer),\n",
    "    \"decisionAccuracy\": (HI.AI_Assisted_DecisionMaking, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraint using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresTrustCalibration))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the AI_Assisted_DecisionMaking class\n",
    "g.add((HI.AI_Assisted_DecisionMaking, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances\n",
    "new_instances = {\n",
    "    \"CaseStudy_TrustAI\": {\n",
    "        \"type\": HI.AI_Assisted_DecisionMaking,\n",
    "        \"decisionAccuracy\": 0.85\n",
    "    },\n",
    "    \"Experiment_OverReliance\": {\n",
    "        \"type\": HI.Reliance_Behavior,\n",
    "        \"adherenceLevel\": 90\n",
    "    },\n",
    "    \"Study_TrustCalibration\": {\n",
    "        \"type\": HI.TrustCalibration,\n",
    "        \"trustScore\": 0.75\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add instances\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.integer)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the third PDF\n",
    "new_classes = {\n",
    "    \"Clinical_AI\": HI.AIConcept,\n",
    "    \"Trust_in_Healthcare_AI\": HI.Clinical_AI,\n",
    "    \"AI_Deployment\": HI.Clinical_AI,\n",
    "    \"RegulatoryCompliance\": HI.AI_Deployment,\n",
    "    \"Human_Factors\": HI.Trust_in_Healthcare_AI,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the third PDF\n",
    "new_object_properties = {\n",
    "    \"requiresRegulatoryApproval\": (HI.AI_Deployment, HI.RegulatoryCompliance),\n",
    "    \"influencesTrust\": (HI.Human_Factors, HI.Trust_in_Healthcare_AI),\n",
    "    \"isUsedInClinicalContext\": (HI.Clinical_AI, HI.AI_Deployment),\n",
    "    \"hasRiskFactor\": (HI.Trust_in_Healthcare_AI, HI.Human_Factors),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the third PDF\n",
    "new_datatype_properties = {\n",
    "    \"complianceScore\": (HI.RegulatoryCompliance, XSD.float),\n",
    "    \"trustLevel\": (HI.Trust_in_Healthcare_AI, XSD.float),\n",
    "    \"errorRate\": (HI.AI_Deployment, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresRegulatoryApproval))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the AI_Deployment class\n",
    "g.add((HI.AI_Deployment, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to third PDF concepts\n",
    "new_instances = {\n",
    "    \"FDA_Approval_Process\": {\n",
    "        \"type\": HI.RegulatoryCompliance,\n",
    "        \"complianceScore\": 0.9\n",
    "    },\n",
    "    \"Study_Trust_in_AI\": {\n",
    "        \"type\": HI.Trust_in_Healthcare_AI,\n",
    "        \"trustLevel\": 0.7\n",
    "    },\n",
    "    \"Clinical_Trial_Study\": {\n",
    "        \"type\": HI.AI_Deployment,\n",
    "        \"errorRate\": 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.integer)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the fourth PDF\n",
    "new_classes = {\n",
    "    \"AI_Ethics_Compliance\": HI.AI_Deployment,  # Adding as a subclass under AI_Deployment\n",
    "    \"Explainable_AI\": HI.AIConcept,\n",
    "    \"Bias_in_AI\": HI.AI_Ethics_Compliance,\n",
    "    \"Fairness_Metrics\": HI.Bias_in_AI,\n",
    "    \"Human_Trust_Factors\": HI.Trust_in_Healthcare_AI,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the fourth PDF\n",
    "new_object_properties = {\n",
    "    \"evaluatesBias\": (HI.AI_Ethics_Compliance, HI.Bias_in_AI),\n",
    "    \"usesFairnessMetric\": (HI.Bias_in_AI, HI.Fairness_Metrics),\n",
    "    \"affectsHumanTrust\": (HI.Human_Trust_Factors, HI.Trust_in_Healthcare_AI),\n",
    "    \"requiresExplainability\": (HI.AI_Deployment, HI.Explainable_AI),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the fourth PDF\n",
    "new_datatype_properties = {\n",
    "    \"biasScore\": (HI.Bias_in_AI, XSD.float),\n",
    "    \"fairnessIndex\": (HI.Fairness_Metrics, XSD.float),\n",
    "    \"trustScore\": (HI.Human_Trust_Factors, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresExplainability))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the AI_Deployment class\n",
    "g.add((HI.AI_Deployment, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to fourth PDF concepts\n",
    "new_instances = {\n",
    "    \"XAI_Research_Study\": {\n",
    "        \"type\": HI.Explainable_AI,\n",
    "        \"requiresExplainability\": True\n",
    "    },\n",
    "    \"Fairness_Audit_Report\": {\n",
    "        \"type\": HI.Fairness_Metrics,\n",
    "        \"fairnessIndex\": 0.85\n",
    "    },\n",
    "    \"Bias_Detection_Tool\": {\n",
    "        \"type\": HI.Bias_in_AI,\n",
    "        \"biasScore\": 0.12\n",
    "    },\n",
    "    \"Trust_Survey\": {\n",
    "        \"type\": HI.Human_Trust_Factors,\n",
    "        \"trustScore\": 0.78\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the fifth PDF\n",
    "new_classes = {\n",
    "    \"Human_AI_Collaboration\": HI.AIConcept,\n",
    "    \"Interactive_Machine_Learning\": HI.Human_AI_Collaboration,\n",
    "    \"Trust_Calibration_Methods\": HI.Human_AI_Collaboration,\n",
    "    \"Human_Centered_Design\": HI.AI_Ethics_Compliance,\n",
    "    \"AI_Safety_Measures\": HI.AI_Ethics_Compliance,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the fifth PDF\n",
    "new_object_properties = {\n",
    "    \"involvesHumanInteraction\": (HI.Human_AI_Collaboration, HI.Human),\n",
    "    \"requiresTrustCalibration\": (HI.Human_AI_Collaboration, HI.Trust_Calibration_Methods),\n",
    "    \"usesInteractiveLearning\": (HI.Human_AI_Collaboration, HI.Interactive_Machine_Learning),\n",
    "    \"followsHumanCenteredDesign\": (HI.AI_Safety_Measures, HI.Human_Centered_Design),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the fifth PDF\n",
    "new_datatype_properties = {\n",
    "    \"trustCalibrationScore\": (HI.Trust_Calibration_Methods, XSD.float),\n",
    "    \"humanInteractionLevel\": (HI.Human_AI_Collaboration, XSD.float),\n",
    "    \"safetyComplianceRate\": (HI.AI_Safety_Measures, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresTrustCalibration))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the Human_AI_Collaboration class\n",
    "g.add((HI.Human_AI_Collaboration, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to fifth PDF concepts\n",
    "new_instances = {\n",
    "    \"Collaborative_AI_Model\": {\n",
    "        \"type\": HI.Human_AI_Collaboration,\n",
    "        \"humanInteractionLevel\": 0.75\n",
    "    },\n",
    "    \"Trust_Benchmarking_Study\": {\n",
    "        \"type\": HI.Trust_Calibration_Methods,\n",
    "        \"trustCalibrationScore\": 0.82\n",
    "    },\n",
    "    \"AI_Ethics_Compliance_Report\": {\n",
    "        \"type\": HI.AI_Safety_Measures,\n",
    "        \"safetyComplianceRate\": 0.91\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the sixth PDF\n",
    "new_classes = {\n",
    "    \"Explainable_AI\": HI.AIConcept,\n",
    "    \"AI_Fairness\": HI.AIConcept,\n",
    "    \"Bias_Mitigation_Strategy\": HI.AI_Fairness,\n",
    "    \"Causal_Inference\": HI.Explainable_AI,\n",
    "    \"Interpretable_Models\": HI.Explainable_AI,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the sixth PDF\n",
    "new_object_properties = {\n",
    "    \"improvesTransparency\": (HI.Explainable_AI, HI.Human),\n",
    "    \"hasBiasMitigation\": (HI.AI_Fairness, HI.Bias_Mitigation_Strategy),\n",
    "    \"usesCausalInference\": (HI.Explainable_AI, HI.Causal_Inference),\n",
    "    \"enhancesInterpretability\": (HI.Explainable_AI, HI.Interpretable_Models),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the sixth PDF\n",
    "new_datatype_properties = {\n",
    "    \"transparencyScore\": (HI.Explainable_AI, XSD.float),\n",
    "    \"biasReductionRate\": (HI.AI_Fairness, XSD.float),\n",
    "    \"interpretabilityLevel\": (HI.Interpretable_Models, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.hasBiasMitigation))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the AI_Fairness class\n",
    "g.add((HI.AI_Fairness, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to sixth PDF concepts\n",
    "new_instances = {\n",
    "    \"XAI_Model\": {\n",
    "        \"type\": HI.Explainable_AI,\n",
    "        \"transparencyScore\": 0.85\n",
    "    },\n",
    "    \"Bias_Correction_Algorithm\": {\n",
    "        \"type\": HI.Bias_Mitigation_Strategy,\n",
    "        \"biasReductionRate\": 0.76\n",
    "    },\n",
    "    \"Causal_Explainability_Framework\": {\n",
    "        \"type\": HI.Causal_Inference,\n",
    "        \"interpretabilityLevel\": 0.91\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the seventh PDF\n",
    "new_classes = {\n",
    "    \"Human_in_Loop\": HI.AIConcept,\n",
    "    \"Interactive_ML\": HI.Human_in_Loop,\n",
    "    \"Trust_Adaptation\": HI.Human_in_Loop,\n",
    "    \"Reinforcement_Learning\": HI.AIConcept,\n",
    "    \"Reward_Shaping\": HI.Reinforcement_Learning,\n",
    "    \"Interactive_Rewards\": HI.Reinforcement_Learning,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the seventh PDF\n",
    "new_object_properties = {\n",
    "    \"requiresHumanInput\": (HI.Human_in_Loop, HI.Human),\n",
    "    \"adaptsTrustLevel\": (HI.Trust_Adaptation, HI.AIConcept),\n",
    "    \"optimizesRewards\": (HI.Reinforcement_Learning, HI.Reward_Shaping),\n",
    "    \"hasInteractiveRewards\": (HI.Reward_Shaping, HI.Interactive_Rewards),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the seventh PDF\n",
    "new_datatype_properties = {\n",
    "    \"trustLevelScore\": (HI.Trust_Adaptation, XSD.float),\n",
    "    \"rewardOptimizationFactor\": (HI.Reward_Shaping, XSD.float),\n",
    "    \"humanFeedbackReliability\": (HI.Interactive_Rewards, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresHumanInput))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the Human_in_Loop class\n",
    "g.add((HI.Human_in_Loop, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to seventh PDF concepts\n",
    "new_instances = {\n",
    "    \"Human_Guided_RL\": {\n",
    "        \"type\": HI.Human_in_Loop,\n",
    "        \"trustLevelScore\": 0.78\n",
    "    },\n",
    "    \"Adaptive_Trust_System\": {\n",
    "        \"type\": HI.Trust_Adaptation,\n",
    "        \"trustLevelScore\": 0.85\n",
    "    },\n",
    "    \"Reinforcement_Agent\": {\n",
    "        \"type\": HI.Reinforcement_Learning,\n",
    "        \"rewardOptimizationFactor\": 0.92\n",
    "    },\n",
    "    \"Interactive_Feedback_Model\": {\n",
    "        \"type\": HI.Interactive_Rewards,\n",
    "        \"humanFeedbackReliability\": 0.88\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the eighth PDF\n",
    "new_classes = {\n",
    "    \"AI_Assisted_DecisionMaking\": HI.AIConcept,\n",
    "    \"Reliance_Behavior\": HI.AI_Assisted_DecisionMaking,\n",
    "    \"Human_AI_Interaction\": HI.AIConcept,\n",
    "    \"Accuracy_Assessment\": HI.Human_AI_Interaction,\n",
    "    \"Trust_Measurement\": HI.Human_AI_Interaction,\n",
    "    \"Decision_Support\": HI.AIConcept,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the eighth PDF\n",
    "new_object_properties = {\n",
    "    \"requiresTrustCalibration\": (HI.AI_Assisted_DecisionMaking, HI.Trust_Measurement),\n",
    "    \"impactsDecisionAccuracy\": (HI.Reliance_Behavior, HI.Accuracy_Assessment),\n",
    "    \"interactsWithHuman\": (HI.Human_AI_Interaction, HI.Human),\n",
    "    \"supportsDecisionMaking\": (HI.Decision_Support, HI.AI_Assisted_DecisionMaking),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the eighth PDF\n",
    "new_datatype_properties = {\n",
    "    \"trustScore\": (HI.Trust_Measurement, XSD.float),\n",
    "    \"accuracyImprovement\": (HI.Accuracy_Assessment, XSD.float),\n",
    "    \"decisionConfidence\": (HI.Decision_Support, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresTrustCalibration))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the AI_Assisted_DecisionMaking class\n",
    "g.add((HI.AI_Assisted_DecisionMaking, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to eighth PDF concepts\n",
    "new_instances = {\n",
    "    \"Trust_Evaluation_Model\": {\n",
    "        \"type\": HI.Trust_Measurement,\n",
    "        \"trustScore\": 0.83\n",
    "    },\n",
    "    \"AI_Decision_Support_Tool\": {\n",
    "        \"type\": HI.Decision_Support,\n",
    "        \"decisionConfidence\": 0.91\n",
    "    },\n",
    "    \"Human_Reliance_Study\": {\n",
    "        \"type\": HI.Reliance_Behavior,\n",
    "        \"accuracyImprovement\": 0.78\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the ninth PDF\n",
    "new_classes = {\n",
    "    \"Clinical_AI\": HI.AIConcept,\n",
    "    \"Trust_in_AI\": HI.Clinical_AI,\n",
    "    \"Trust_Calibration\": HI.Trust_in_AI,\n",
    "    \"AI_Deployment_Process\": HI.Clinical_AI,\n",
    "    \"Healthcare_Workflow\": HI.Clinical_AI,\n",
    "    \"Human_AI_Collaboration\": HI.AIConcept,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the ninth PDF\n",
    "new_object_properties = {\n",
    "    \"affectsTrust\": (HI.Trust_Calibration, HI.Trust_in_AI),\n",
    "    \"isIntegratedIn\": (HI.AI_Deployment_Process, HI.Healthcare_Workflow),\n",
    "    \"requiresHumanSupervision\": (HI.Human_AI_Collaboration, HI.Clinical_AI),\n",
    "    \"involvesDecisionSupport\": (HI.Human_AI_Collaboration, HI.Trust_Calibration),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the ninth PDF\n",
    "new_datatype_properties = {\n",
    "    \"trustScore\": (HI.Trust_Calibration, XSD.float),\n",
    "    \"workflowEfficiency\": (HI.Healthcare_Workflow, XSD.float),\n",
    "    \"humanInvolvementLevel\": (HI.Human_AI_Collaboration, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.requiresHumanSupervision))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the Human_AI_Collaboration class\n",
    "g.add((HI.Human_AI_Collaboration, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to ninth PDF concepts\n",
    "new_instances = {\n",
    "    \"AI_Trust_Model\": {\n",
    "        \"type\": HI.Trust_Calibration,\n",
    "        \"trustScore\": 0.87\n",
    "    },\n",
    "    \"Hospital_Workflow_Optimization\": {\n",
    "        \"type\": HI.Healthcare_Workflow,\n",
    "        \"workflowEfficiency\": 0.92\n",
    "    },\n",
    "    \"AI_Human_Team\": {\n",
    "        \"type\": HI.Human_AI_Collaboration,\n",
    "        \"humanInvolvementLevel\": 0.78\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ontology updated and saved to: updated_hi_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, Literal, RDF, RDFS, OWL, XSD, BNode\n",
    "\n",
    "# Load the existing ontology\n",
    "ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"turtle\")\n",
    "\n",
    "# Define correct namespace (keeping it consistent)\n",
    "HI = Namespace(\"http://www.semanticweb.org/vbr240/ontologies/2022/4/untitled-ontology-51/\")\n",
    "g.bind(\"hi\", HI)\n",
    "\n",
    "# Define new Classes from the tenth PDF\n",
    "new_classes = {\n",
    "    \"Value_Aware_Learning\": HI.AIConcept,\n",
    "    \"Selective_Classification\": HI.Value_Aware_Learning,\n",
    "    \"Active_Learning_Strategy\": HI.Value_Aware_Learning,\n",
    "    \"Threshold_Oriented_Sampling\": HI.Active_Learning_Strategy,\n",
    "    \"Human_Agent_Interaction\": HI.AIConcept,\n",
    "    \"Trust_Aware_DecisionMaking\": HI.Human_Agent_Interaction,\n",
    "}\n",
    "\n",
    "# Add new classes to the ontology\n",
    "for cls, parent in new_classes.items():\n",
    "    g.add((HI[cls], RDF.type, OWL.Class))\n",
    "    g.add((HI[cls], RDFS.subClassOf, parent))\n",
    "\n",
    "# Define new Object Properties from the tenth PDF\n",
    "new_object_properties = {\n",
    "    \"improvesDecisionMaking\": (HI.Trust_Aware_DecisionMaking, HI.Human_Agent_Interaction),\n",
    "    \"isUsedIn\": (HI.Threshold_Oriented_Sampling, HI.Active_Learning_Strategy),\n",
    "    \"enhancesSelectiveLearning\": (HI.Selective_Classification, HI.Value_Aware_Learning),\n",
    "    \"appliesThresholding\": (HI.Threshold_Oriented_Sampling, HI.Selective_Classification),\n",
    "}\n",
    "\n",
    "# Add Object Properties\n",
    "for prop, (domain, range_) in new_object_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.ObjectProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, range_))\n",
    "\n",
    "# Define new Datatype Properties from the tenth PDF\n",
    "new_datatype_properties = {\n",
    "    \"learningEfficiency\": (HI.Active_Learning_Strategy, XSD.float),\n",
    "    \"trustLevel\": (HI.Trust_Aware_DecisionMaking, XSD.float),\n",
    "    \"classificationConfidence\": (HI.Selective_Classification, XSD.float),\n",
    "}\n",
    "\n",
    "# Add Datatype Properties\n",
    "for prop, (domain, dtype) in new_datatype_properties.items():\n",
    "    g.add((HI[prop], RDF.type, OWL.DatatypeProperty))\n",
    "    g.add((HI[prop], RDFS.domain, domain))\n",
    "    g.add((HI[prop], RDFS.range, dtype))\n",
    "\n",
    "# Define minimum cardinality constraints using a blank node\n",
    "restriction_node = BNode()\n",
    "g.add((restriction_node, RDF.type, OWL.Restriction))\n",
    "g.add((restriction_node, OWL.onProperty, HI.enhancesSelectiveLearning))\n",
    "g.add((restriction_node, OWL.minCardinality, Literal(1, datatype=XSD.nonNegativeInteger)))\n",
    "\n",
    "# Attach the restriction to the Selective_Classification class\n",
    "g.add((HI.Selective_Classification, RDFS.subClassOf, restriction_node))\n",
    "\n",
    "# Define new Instances related to tenth PDF concepts\n",
    "new_instances = {\n",
    "    \"Threshold_Sampling_Model\": {\n",
    "        \"type\": HI.Threshold_Oriented_Sampling,\n",
    "        \"learningEfficiency\": 0.89\n",
    "    },\n",
    "    \"Human_Agent_Trust_Model\": {\n",
    "        \"type\": HI.Trust_Aware_DecisionMaking,\n",
    "        \"trustLevel\": 0.92\n",
    "    },\n",
    "    \"Selective_Learning_System\": {\n",
    "        \"type\": HI.Selective_Classification,\n",
    "        \"classificationConfidence\": 0.87\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add instances to the ontology\n",
    "for instance, attributes in new_instances.items():\n",
    "    g.add((HI[instance], RDF.type, attributes[\"type\"]))\n",
    "    for attr, value in attributes.items():\n",
    "        if attr != \"type\":  # Skip 'type' since it's already added\n",
    "            g.add((HI[instance], HI[attr], Literal(value, datatype=XSD.float if isinstance(value, float) else XSD.boolean)))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"updated_hi_ontology.ttl\"\n",
    "g.serialize(updated_ontology_path, format=\"turtle\")\n",
    "\n",
    "print(f\"‚úÖ Ontology updated and saved to: {updated_ontology_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evoman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
